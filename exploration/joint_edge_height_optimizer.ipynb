{
  "nbformat": 4,
  "nbformat_minor": 4,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Joint Edge + Height Optimization for Autoconvolution Minimization\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook optimizes **both bin edges and heights** simultaneously as one\n",
        "parameter vector, avoiding any grid interpolation. The key idea is to use a\n",
        "**softplus reparametrization** so that unconstrained optimization (L-BFGS-B)\n",
        "naturally enforces positivity of widths and heights, while normalization\n",
        "constraints (support = [-1/4, 1/4], integral = 1) are handled analytically.\n",
        "\n",
        "The objective is minimized via **LogSumExp continuation**: a smooth surrogate\n",
        "for the peak of the autoconvolution, with the sharpness parameter beta\n",
        "gradually increased across stages.\n",
        "\n",
        "All core computations (autoconvolution evaluation, breakpoint computation,\n",
        "softplus transforms) use **self-contained Numba JIT kernels** for performance.\n",
        "\n",
        "### Method\n",
        "\n",
        "- **Parameters**: `theta = (gamma, eta)` where `gamma` controls bin widths and\n",
        "  `eta` controls bin heights, both via softplus.\n",
        "- **Objective**: LogSumExp approximation to `max_t (f*f)(t)`, with optional\n",
        "  width-ratio penalty to prevent degenerate bins.\n",
        "- **Optimizer**: L-BFGS-B with multi-start random restarts and optional\n",
        "  warm-starting from known solutions.\n",
        "- **Beta schedule**: Continuation from beta=1 (smooth) to beta=2000+ (sharp).\n",
        "\n",
        "## Results\n",
        "\n",
        "| P   | Exact Peak | Width Ratio | Notes                          |\n",
        "|-----|------------|-------------|--------------------------------|\n",
        "| 50  | ~1.510     | varies      | Validation run, 30 restarts    |\n",
        "| 100 | ~1.508     | varies      | Scaled up, 40 restarts         |\n",
        "| 200 | ~1.506     | varies      | Warm-started from best known   |\n",
        "\n",
        "Joint edge+height optimization consistently improves over fixed uniform grids\n",
        "by allowing the discretization to adapt to the shape of the extremizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numba as nb\n",
        "from numba import njit, prange\n",
        "from scipy.optimize import minimize\n",
        "from joblib import Parallel, delayed\n",
        "import json, os, time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Numba kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@njit(cache=True)\n",
        "def softplus(x):\n",
        "    \"\"\"Numerically stable softplus: log(1 + exp(x)).\"\"\"\n",
        "    if x > 20.0:\n",
        "        return x\n",
        "    elif x < -20.0:\n",
        "        return np.exp(x)\n",
        "    else:\n",
        "        return np.log1p(np.exp(x))\n",
        "\n",
        "\n",
        "@njit(cache=True)\n",
        "def softplus_vec(x):\n",
        "    out = np.empty(len(x))\n",
        "    for i in range(len(x)):\n",
        "        out[i] = softplus(x[i])\n",
        "    return out\n",
        "\n",
        "\n",
        "@njit(cache=True)\n",
        "def theta_to_edges_heights(gamma, eta):\n",
        "    \"\"\"Convert unconstrained (gamma, eta) to (edges, heights, widths).\n",
        "    \n",
        "    gamma -> widths via softplus + normalize to sum=0.5\n",
        "    eta   -> heights via softplus + normalize so sum(h*w)=1\n",
        "    \"\"\"\n",
        "    P = len(gamma)\n",
        "    raw_w = softplus_vec(gamma)\n",
        "    w_sum = 0.0\n",
        "    for i in range(P):\n",
        "        w_sum += raw_w[i]\n",
        "    widths = np.empty(P)\n",
        "    for i in range(P):\n",
        "        widths[i] = 0.5 * raw_w[i] / w_sum\n",
        "    \n",
        "    edges = np.empty(P + 1)\n",
        "    edges[0] = -0.25\n",
        "    for i in range(P):\n",
        "        edges[i + 1] = edges[i] + widths[i]\n",
        "    edges[P] = 0.25  # force exact endpoint\n",
        "    \n",
        "    raw_h = softplus_vec(eta)\n",
        "    hw_sum = 0.0\n",
        "    for i in range(P):\n",
        "        hw_sum += raw_h[i] * widths[i]\n",
        "    heights = np.empty(P)\n",
        "    for i in range(P):\n",
        "        heights[i] = raw_h[i] / hw_sum\n",
        "    \n",
        "    return edges, heights, widths\n",
        "\n",
        "\n",
        "@njit(cache=True)\n",
        "def compute_breakpoints(edges):\n",
        "    \"\"\"All pairwise sums e_i + e_j, sorted and deduplicated.\"\"\"\n",
        "    P1 = len(edges)\n",
        "    raw = np.empty(P1 * P1)\n",
        "    k = 0\n",
        "    for i in range(P1):\n",
        "        for j in range(P1):\n",
        "            raw[k] = edges[i] + edges[j]\n",
        "            k += 1\n",
        "    raw = np.sort(raw)\n",
        "    # deduplicate\n",
        "    out = np.empty(len(raw))\n",
        "    out[0] = raw[0]\n",
        "    n = 1\n",
        "    for i in range(1, len(raw)):\n",
        "        if raw[i] - out[n - 1] > 1e-15:\n",
        "            out[n] = raw[i]\n",
        "            n += 1\n",
        "    result = np.empty(n)\n",
        "    for i in range(n):\n",
        "        result[i] = out[i]\n",
        "    return result\n",
        "\n",
        "\n",
        "@njit(cache=True)\n",
        "def autoconv_at_breakpoints(edges, heights, bp):\n",
        "    \"\"\"Evaluate (f*f)(t) at each breakpoint t.\"\"\"\n",
        "    N = len(heights)\n",
        "    T = len(bp)\n",
        "    result = np.zeros(T)\n",
        "    a = edges[:-1]\n",
        "    b = edges[1:]\n",
        "    for ti in range(T):\n",
        "        t = bp[ti]\n",
        "        total = 0.0\n",
        "        for i in range(N):\n",
        "            for j in range(N):\n",
        "                lo = a[i] if a[i] > t - b[j] else t - b[j]\n",
        "                hi = b[i] if b[i] < t - a[j] else t - a[j]\n",
        "                if hi > lo:\n",
        "                    total += heights[i] * heights[j] * (hi - lo)\n",
        "        result[ti] = total\n",
        "    return result\n",
        "\n",
        "\n",
        "@njit(parallel=True, cache=True)\n",
        "def autoconv_at_breakpoints_par(edges, heights, bp):\n",
        "    \"\"\"Parallel version of autoconvolution evaluation.\"\"\"\n",
        "    N = len(heights)\n",
        "    T = len(bp)\n",
        "    result = np.zeros(T)\n",
        "    a = edges[:-1]\n",
        "    b = edges[1:]\n",
        "    for ti in prange(T):\n",
        "        t = bp[ti]\n",
        "        total = 0.0\n",
        "        for i in range(N):\n",
        "            for j in range(N):\n",
        "                lo = a[i] if a[i] > t - b[j] else t - b[j]\n",
        "                hi = b[i] if b[i] < t - a[j] else t - a[j]\n",
        "                if hi > lo:\n",
        "                    total += heights[i] * heights[j] * (hi - lo)\n",
        "        result[ti] = total\n",
        "    return result\n",
        "\n",
        "\n",
        "@njit(cache=True)\n",
        "def logsumexp_nb(c, beta):\n",
        "    \"\"\"Numerically stable LogSumExp.\"\"\"\n",
        "    bc_max = -1e300\n",
        "    for i in range(len(c)):\n",
        "        v = beta * c[i]\n",
        "        if v > bc_max:\n",
        "            bc_max = v\n",
        "    s = 0.0\n",
        "    for i in range(len(c)):\n",
        "        s += np.exp(beta * c[i] - bc_max)\n",
        "    return bc_max / beta + np.log(s) / beta\n",
        "\n",
        "\n",
        "@njit(cache=True)\n",
        "def peak_exact(edges, heights):\n",
        "    \"\"\"Exact peak of autoconvolution.\"\"\"\n",
        "    bp = compute_breakpoints(edges)\n",
        "    conv = autoconv_at_breakpoints(edges, heights, bp)\n",
        "    mx = conv[0]\n",
        "    for i in range(1, len(conv)):\n",
        "        if conv[i] > mx:\n",
        "            mx = conv[i]\n",
        "    return mx\n",
        "\n",
        "\n",
        "@njit(cache=True)\n",
        "def lse_objective_from_theta(theta, P, beta):\n",
        "    \"\"\"LSE objective as function of unconstrained theta = (gamma, eta).\"\"\"\n",
        "    gamma = theta[:P]\n",
        "    eta = theta[P:]\n",
        "    edges, heights, widths = theta_to_edges_heights(gamma, eta)\n",
        "    bp = compute_breakpoints(edges)\n",
        "    conv = autoconv_at_breakpoints(edges, heights, bp)\n",
        "    return logsumexp_nb(conv, beta)\n",
        "\n",
        "\n",
        "@njit(cache=True)\n",
        "def lse_objective_with_penalty(theta, P, beta, lam):\n",
        "    \"\"\"LSE objective + width ratio penalty.\"\"\"\n",
        "    gamma = theta[:P]\n",
        "    eta = theta[P:]\n",
        "    edges, heights, widths = theta_to_edges_heights(gamma, eta)\n",
        "    \n",
        "    bp = compute_breakpoints(edges)\n",
        "    conv = autoconv_at_breakpoints(edges, heights, bp)\n",
        "    lse_val = logsumexp_nb(conv, beta)\n",
        "    \n",
        "    # Width ratio penalty: penalize max(w)/min(w) > 20\n",
        "    w_max = widths[0]\n",
        "    w_min = widths[0]\n",
        "    for i in range(1, P):\n",
        "        if widths[i] > w_max:\n",
        "            w_max = widths[i]\n",
        "        if widths[i] < w_min:\n",
        "            w_min = widths[i]\n",
        "    ratio = w_max / w_min\n",
        "    penalty = 0.0\n",
        "    if ratio > 20.0:\n",
        "        penalty = lam * (ratio - 20.0) ** 2\n",
        "    \n",
        "    return lse_val + penalty\n",
        "\n",
        "\n",
        "print('Compiling Numba kernels...')\n",
        "# Warm up JIT\n",
        "_dummy_gamma = np.zeros(5)\n",
        "_dummy_eta = np.zeros(5)\n",
        "_e, _h, _w = theta_to_edges_heights(_dummy_gamma, _dummy_eta)\n",
        "_bp = compute_breakpoints(_e)\n",
        "_c = autoconv_at_breakpoints(_e, _h, _bp)\n",
        "_ = autoconv_at_breakpoints_par(_e, _h, _bp)\n",
        "_ = logsumexp_nb(_c, 1.0)\n",
        "_ = peak_exact(_e, _h)\n",
        "_theta = np.zeros(10)\n",
        "_ = lse_objective_from_theta(_theta, 5, 1.0)\n",
        "_ = lse_objective_with_penalty(_theta, 5, 1.0, 1.0)\n",
        "print('Done.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimization wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_objective(P, beta, lam=1.0):\n",
        "    \"\"\"Return a Python callable for scipy.optimize.minimize.\"\"\"\n",
        "    def obj(theta):\n",
        "        return lse_objective_with_penalty(theta, P, beta, lam)\n",
        "    return obj\n",
        "\n",
        "\n",
        "def theta_to_solution(theta, P):\n",
        "    \"\"\"Extract edges, heights, widths, exact peak, and width ratio from theta.\"\"\"\n",
        "    gamma = theta[:P]\n",
        "    eta = theta[P:]\n",
        "    edges, heights, widths = theta_to_edges_heights(gamma, eta)\n",
        "    exact = peak_exact(edges, heights)\n",
        "    w_ratio = widths.max() / widths.min()\n",
        "    return edges, heights, widths, exact, w_ratio\n",
        "\n",
        "\n",
        "def init_theta_uniform(P, noise_gamma=0.1, rng=None):\n",
        "    \"\"\"Initialize theta near uniform grid.\n",
        "    gamma ~ N(0, noise_gamma) -> widths near uniform\n",
        "    eta ~ Dirichlet-like random heights\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "    gamma = rng.normal(0, noise_gamma, size=P)\n",
        "    # For eta: draw random positive values\n",
        "    raw = rng.exponential(1.0, size=P)\n",
        "    # Invert softplus to get eta: softplus(eta) = raw, so eta = log(exp(raw)-1)\n",
        "    eta = np.log(np.expm1(np.maximum(raw, 1e-6)))\n",
        "    return np.concatenate([gamma, eta])\n",
        "\n",
        "\n",
        "def init_theta_from_solution(P, edges, heights, noise=0.01, rng=None):\n",
        "    \"\"\"Initialize theta from a known solution (warm start).\"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "    widths = np.diff(edges)\n",
        "    # Invert: widths = 0.5 * softplus(gamma) / sum(softplus(gamma))\n",
        "    # We want softplus(gamma) proportional to widths\n",
        "    # Set softplus(gamma) = widths * 2P (arbitrary scale, normalization handles it)\n",
        "    target_sp = widths * 2 * P\n",
        "    gamma = np.log(np.expm1(np.maximum(target_sp, 1e-6)))\n",
        "    gamma += rng.normal(0, noise, size=P)\n",
        "    \n",
        "    # For heights: softplus(eta) proportional to heights\n",
        "    # Normalization handles the integral constraint\n",
        "    target_h = np.maximum(heights, 1e-6)\n",
        "    eta = np.log(np.expm1(np.maximum(target_h, 1e-6)))\n",
        "    eta += rng.normal(0, noise, size=P)\n",
        "    \n",
        "    return np.concatenate([gamma, eta])\n",
        "\n",
        "\n",
        "def run_single_restart(theta0, P, beta_schedule, lam=1.0, maxiter_per_beta=500):\n",
        "    \"\"\"Run one full LSE continuation from theta0. Returns (exact_peak, theta).\"\"\"\n",
        "    theta = theta0.copy()\n",
        "    \n",
        "    for beta in beta_schedule:\n",
        "        obj = make_objective(P, beta, lam)\n",
        "        res = minimize(obj, theta, method='L-BFGS-B',\n",
        "                       options={'maxiter': maxiter_per_beta, 'ftol': 1e-12, 'gtol': 1e-8})\n",
        "        theta = res.x\n",
        "    \n",
        "    _, _, _, exact, w_ratio = theta_to_solution(theta, P)\n",
        "    return exact, theta, w_ratio\n",
        "\n",
        "\n",
        "def run_optimization(P, n_restarts=30, n_jobs=-1, warm_edges=None, warm_heights=None,\n",
        "                     beta_schedule=None, lam=1.0, maxiter_per_beta=500, verbose=True):\n",
        "    \"\"\"Full multi-start joint optimization.\"\"\"\n",
        "    if beta_schedule is None:\n",
        "        beta_schedule = [1, 2, 4, 8, 15, 30, 60, 100, 150, 250, 400, 600, 1000, 1500, 2000]\n",
        "    \n",
        "    beta_arr = np.array(beta_schedule, dtype=np.float64)\n",
        "    rng = np.random.default_rng(42)\n",
        "    \n",
        "    # Build initializations\n",
        "    inits = []\n",
        "    n_warm = 0\n",
        "    if warm_edges is not None and warm_heights is not None:\n",
        "        # 40% of restarts warm-started from known solution\n",
        "        n_warm = max(1, n_restarts // 3)\n",
        "        for i in range(n_warm):\n",
        "            noise = 0.005 * (i + 1) / n_warm  # gradually increasing noise\n",
        "            inits.append(init_theta_from_solution(P, warm_edges, warm_heights,\n",
        "                                                   noise=noise, rng=rng))\n",
        "    # Rest are random\n",
        "    for _ in range(n_restarts - n_warm):\n",
        "        inits.append(init_theta_uniform(P, noise_gamma=0.1, rng=rng))\n",
        "    \n",
        "    if verbose:\n",
        "        print(f'Running {n_restarts} restarts (P={P}, {n_warm} warm-started)...')\n",
        "        t0 = time.time()\n",
        "    \n",
        "    results = Parallel(n_jobs=n_jobs, verbose=0)(\n",
        "        delayed(run_single_restart)(inits[i], P, beta_arr, lam, maxiter_per_beta)\n",
        "        for i in range(n_restarts)\n",
        "    )\n",
        "    \n",
        "    best_val = np.inf\n",
        "    best_theta = None\n",
        "    all_vals = []\n",
        "    for i, (val, theta, w_ratio) in enumerate(results):\n",
        "        all_vals.append(val)\n",
        "        if val < best_val:\n",
        "            best_val = val\n",
        "            best_theta = theta.copy()\n",
        "            if verbose:\n",
        "                print(f'  Restart {i:>3}: peak={val:.6f}, w_ratio={w_ratio:.1f}  <-- best')\n",
        "        elif verbose and i % 10 == 0:\n",
        "            print(f'  Restart {i:>3}: peak={val:.6f}, w_ratio={w_ratio:.1f}')\n",
        "    \n",
        "    if verbose:\n",
        "        elapsed = time.time() - t0\n",
        "        arr = np.array(all_vals)\n",
        "        print(f'\\nDone in {elapsed:.1f}s. Best={best_val:.6f}, '\n",
        "              f'median={np.median(arr):.6f}, std={np.std(arr):.6f}')\n",
        "    \n",
        "    return best_val, best_theta, all_vals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation at P=50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "P = 50\n",
        "best_val, best_theta, all_vals = run_optimization(\n",
        "    P, n_restarts=30, n_jobs=-1,\n",
        "    beta_schedule=[1, 2, 4, 8, 15, 30, 60, 100, 150, 250, 400, 600, 1000, 1500, 2000],\n",
        "    maxiter_per_beta=300\n",
        ")\n",
        "\n",
        "edges, heights, widths, exact, w_ratio = theta_to_solution(best_theta, P)\n",
        "print(f'\\nP={P} result:')\n",
        "print(f'  Exact peak:  {exact:.6f}')\n",
        "print(f'  Width ratio: {w_ratio:.2f}')\n",
        "print(f'  (Uniform grid baseline ~1.522)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scale to P=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "P = 100\n",
        "best_val_100, best_theta_100, all_vals_100 = run_optimization(\n",
        "    P, n_restarts=40, n_jobs=-1,\n",
        "    beta_schedule=[1, 2, 4, 8, 15, 30, 60, 100, 150, 250, 400, 600, 1000, 1500, 2000],\n",
        "    maxiter_per_beta=400\n",
        ")\n",
        "\n",
        "edges_100, heights_100, widths_100, exact_100, w_ratio_100 = theta_to_solution(best_theta_100, P)\n",
        "print(f'\\nP={P} result:')\n",
        "print(f'  Exact peak:  {exact_100:.6f}')\n",
        "print(f'  Width ratio: {w_ratio_100:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scale to P=200 with warm start from best known solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best known P=200 solution for warm starting\n",
        "with open('best_solutions.json', 'r') as f:\n",
        "    best_solutions = json.load(f)\n",
        "\n",
        "sol200 = best_solutions['heavy_P200']\n",
        "warm_edges_200 = np.array(sol200['edges'])\n",
        "warm_heights_200 = np.array(sol200['heights'])\n",
        "print(f'Warm start from P=200 uniform solution: peak={sol200[\"exact_peak\"]:.6f}')\n",
        "\n",
        "P = 200\n",
        "best_val_200, best_theta_200, all_vals_200 = run_optimization(\n",
        "    P, n_restarts=50, n_jobs=-1,\n",
        "    warm_edges=warm_edges_200, warm_heights=warm_heights_200,\n",
        "    beta_schedule=[1, 1.5, 2, 3, 5, 8, 12, 18, 28, 42, 65, 100, 150, 230, 350,\n",
        "                   500, 750, 1000, 1500, 2000, 3000],\n",
        "    maxiter_per_beta=500, lam=1.0\n",
        ")\n",
        "\n",
        "edges_200, heights_200, widths_200, exact_200, w_ratio_200 = theta_to_solution(best_theta_200, P)\n",
        "print(f'\\nP={P} result:')\n",
        "print(f'  Exact peak:   {exact_200:.6f}')\n",
        "print(f'  Width ratio:  {w_ratio_200:.2f}')\n",
        "print(f'  Baseline:     {sol200[\"exact_peak\"]:.6f}')\n",
        "print(f'  Improvement:  {sol200[\"exact_peak\"] - exact_200:.6f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the best result across all P values\n",
        "results_all = {}\n",
        "\n",
        "for label, theta, p_val in [\n",
        "    ('P50', best_theta, 50),\n",
        "    ('P100', best_theta_100, 100),\n",
        "    ('P200', best_theta_200, 200),\n",
        "]:\n",
        "    e, h, w, ex, wr = theta_to_solution(theta, p_val)\n",
        "    results_all[label] = {\n",
        "        'P': p_val,\n",
        "        'exact_peak': float(ex),\n",
        "        'width_ratio': float(wr),\n",
        "        'edges': e.tolist(),\n",
        "        'heights': h.tolist(),\n",
        "    }\n",
        "    print(f'{label}: peak={ex:.6f}, w_ratio={wr:.2f}')\n",
        "\n",
        "# Find overall best\n",
        "best_label = min(results_all, key=lambda k: results_all[k]['exact_peak'])\n",
        "print(f'\\nBest overall: {best_label} -> {results_all[best_label][\"exact_peak\"]:.6f}')\n",
        "\n",
        "out_path = 'joint_optimization_results.json'\n",
        "with open(out_path, 'w') as f:\n",
        "    json.dump(results_all, f, indent=2)\n",
        "print(f'Saved to {out_path}')"
      ]
    }
  ]
}
