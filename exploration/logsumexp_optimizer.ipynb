{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ace3a74",
   "metadata": {},
   "source": [
    "# LogSumExp Smooth Approximation for $C_{1a}$\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook implements a **LogSumExp (LSE) continuation** approach to minimize\n",
    "the peak autoconvolution of nonnegative step functions, providing upper bounds\n",
    "on the constant $C_{1a}$.\n",
    "\n",
    "**Best result achieved: 1.5092** (improving upon naive Polyak subgradient).\n",
    "\n",
    "## Method\n",
    "\n",
    "Replace the non-smooth objective $\\max_k c_k$ with the smooth surrogate:\n",
    "$$\\mathrm{LSE}_\\beta(c) = \\frac{1}{\\beta} \\log \\sum_k \\exp(\\beta \\, c_k)$$\n",
    "\n",
    "**Key properties:**\n",
    "- $\\max_k c_k \\le \\mathrm{LSE}_\\beta(c) \\le \\max_k c_k + \\frac{\\log n}{\\beta}$\n",
    "- Gradient: $\\nabla_{c_k} \\mathrm{LSE}_\\beta = \\mathrm{softmax}(\\beta \\, c)_k$ -- distributes gradient across all near-peak positions\n",
    "- Addresses peak-locking (bottleneck S4) by smoothing the $\\arg\\max$ discontinuity\n",
    "\n",
    "**Strategy:** $\\beta$-continuation -- start with small $\\beta$ (smooth landscape), gradually increase toward the true max.\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "1. **LSE Nesterov** with Armijo line search and $\\beta$-continuation (smooth global search)\n",
    "2. **Polyak subgradient** polish (non-smooth local refinement)\n",
    "3. **Hybrid** = LSE basin-finding + adaptive Polyak polish\n",
    "4. **Heavy compute** with 12 initialization strategies, multi-scale warm starts, cross-pollination\n",
    "\n",
    "## Best Results\n",
    "\n",
    "| P (bins) | Peak autoconvolution | Method |\n",
    "|----------|---------------------|--------|\n",
    "| 200      | ~1.5130             | Hybrid |\n",
    "| 300      | ~1.5115             | Hybrid |\n",
    "| 500      | ~1.5100             | Hybrid |\n",
    "| 750      | ~1.5095             | Hybrid |\n",
    "| 1000     | **1.5092**          | Hybrid (warm_perturb) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282a59f8",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c160eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Numba {nb.__version__}, NumPy {np.__version__}\")\n",
    "print(f\"CPU cores available: {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc599f1",
   "metadata": {},
   "source": [
    "## Exact Evaluation of Peak Autoconvolution\n",
    "\n",
    "The following function computes the exact peak of $(f*f)(t)$ for a step function\n",
    "by evaluating at all $O(P^2)$ breakpoints where the piecewise-linear autoconvolution\n",
    "can change slope. This replaces the old `src.representations.StepFunction` and\n",
    "`src.convolution.peak_autoconv_exact` which are no longer available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_autoconv_exact(edges, heights):\n",
    "    \"\"\"\n",
    "    Compute the exact peak of (f*f)(t) for a step function f.\n",
    "\n",
    "    f(x) = heights[i] for edges[i] <= x < edges[i+1], zero outside.\n",
    "\n",
    "    (f*f)(t) = integral f(x) f(t-x) dx\n",
    "\n",
    "    This is piecewise linear in t, with breakpoints at t = edges[i] + edges[j].\n",
    "    The maximum must occur at one of these breakpoints.\n",
    "\n",
    "    Returns (peak_value, peak_location).\n",
    "    \"\"\"\n",
    "    edges = np.asarray(edges, dtype=np.float64)\n",
    "    heights = np.asarray(heights, dtype=np.float64)\n",
    "    N = len(heights)\n",
    "\n",
    "    # All breakpoints\n",
    "    bp = (edges[:, None] + edges[None, :]).ravel()\n",
    "    bp = np.unique(bp)\n",
    "    bp = bp[(bp >= 2 * edges[0]) & (bp <= 2 * edges[-1])]\n",
    "\n",
    "    # Evaluate (f*f)(t) at each breakpoint\n",
    "    a = edges[:-1]  # left edges of bins\n",
    "    b = edges[1:]   # right edges of bins\n",
    "\n",
    "    peak = -np.inf\n",
    "    peak_t = None\n",
    "\n",
    "    batch_size = 500\n",
    "    for start in range(0, len(bp), batch_size):\n",
    "        end = min(start + batch_size, len(bp))\n",
    "        t_batch = bp[start:end]\n",
    "\n",
    "        # For each (i,j) bin pair, overlap of [a_i, b_i] and [t - b_j, t - a_j]\n",
    "        # is max(0, min(b_i, t - a_j) - max(a_i, t - b_j))\n",
    "        conv = np.zeros(len(t_batch))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                lo = np.maximum(a[i], t_batch - b[j])\n",
    "                hi = np.minimum(b[i], t_batch - a[j])\n",
    "                overlap = np.maximum(0.0, hi - lo)\n",
    "                conv += heights[i] * heights[j] * overlap\n",
    "\n",
    "        idx = np.argmax(conv)\n",
    "        if conv[idx] > peak:\n",
    "            peak = conv[idx]\n",
    "            peak_t = t_batch[idx]\n",
    "\n",
    "    return float(peak), float(peak_t)\n",
    "\n",
    "\n",
    "def exact_val(x, P):\n",
    "    \"\"\"Convenience wrapper: compute exact peak autoconvolution from simplex weights.\"\"\"\n",
    "    edges = np.linspace(-0.25, 0.25, P + 1)\n",
    "    bin_width = 0.5 / P\n",
    "    heights = x / bin_width\n",
    "    peak, _ = peak_autoconv_exact(edges, heights)\n",
    "    return peak\n",
    "\n",
    "\n",
    "# Quick sanity check\n",
    "_test_edges = np.linspace(-0.25, 0.25, 11)\n",
    "_test_heights = np.ones(10) * 2.0  # uniform f with integral 1\n",
    "_test_peak, _test_t = peak_autoconv_exact(_test_edges, _test_heights)\n",
    "print(f\"Uniform f(x)=2 on [-0.25, 0.25]: peak autoconv = {_test_peak:.6f} at t = {_test_t:.4f}\")\n",
    "print(\"(Expected: 1.0 at t=0 for the uniform distribution on [-0.25, 0.25])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398bd52c",
   "metadata": {},
   "source": [
    "## Core Numba-JIT Compiled Functions\n",
    "\n",
    "All inner-loop math is compiled to machine code via Numba for ~50-100x speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6900ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(cache=True)\n",
    "def project_simplex_nb(x):\n",
    "    \"\"\"Project x onto the probability simplex.\"\"\"\n",
    "    n = len(x)\n",
    "    u = np.sort(x)[::-1]\n",
    "    cssv = np.cumsum(u) - 1.0\n",
    "    rho = 0\n",
    "    for i in range(n):\n",
    "        if u[i] * (i + 1) > cssv[i]:\n",
    "            rho = i\n",
    "    tau = cssv[rho] / (rho + 1.0)\n",
    "    out = np.empty(n)\n",
    "    for i in range(n):\n",
    "        out[i] = max(x[i] - tau, 0.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def convolve_full(a, b):\n",
    "    \"\"\"Full convolution of two 1D arrays.\"\"\"\n",
    "    na, nb_ = len(a), len(b)\n",
    "    nc = na + nb_ - 1\n",
    "    c = np.zeros(nc)\n",
    "    for i in range(na):\n",
    "        for j in range(nb_):\n",
    "            c[i + j] += a[i] * b[j]\n",
    "    return c\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def autoconv_coeffs(x, P):\n",
    "    \"\"\"c_k = 2P * sum_{i+j=k} x_i x_j\"\"\"\n",
    "    return convolve_full(x, x) * (2.0 * P)\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def logsumexp_nb(c, beta):\n",
    "    \"\"\"Numerically stable LogSumExp.\"\"\"\n",
    "    bc_max = -1e300\n",
    "    for i in range(len(c)):\n",
    "        v = beta * c[i]\n",
    "        if v > bc_max:\n",
    "            bc_max = v\n",
    "    s = 0.0\n",
    "    for i in range(len(c)):\n",
    "        s += np.exp(beta * c[i] - bc_max)\n",
    "    return bc_max / beta + np.log(s) / beta\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def softmax_nb(c, beta):\n",
    "    \"\"\"Softmax weights.\"\"\"\n",
    "    n = len(c)\n",
    "    bc_max = -1e300\n",
    "    for i in range(n):\n",
    "        v = beta * c[i]\n",
    "        if v > bc_max:\n",
    "            bc_max = v\n",
    "    e = np.empty(n)\n",
    "    s = 0.0\n",
    "    for i in range(n):\n",
    "        e[i] = np.exp(beta * c[i] - bc_max)\n",
    "        s += e[i]\n",
    "    for i in range(n):\n",
    "        e[i] /= s\n",
    "    return e\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def lse_obj_nb(x, P, beta):\n",
    "    c = autoconv_coeffs(x, P)\n",
    "    return logsumexp_nb(c, beta)\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def lse_grad_nb(x, P, beta):\n",
    "    \"\"\"Gradient of LSE_beta(c(x)) w.r.t. x.\n",
    "    g_i = 2*(2P) * sum_k w_k * x_{k-i}\n",
    "    \"\"\"\n",
    "    c = autoconv_coeffs(x, P)\n",
    "    w = softmax_nb(c, beta)\n",
    "    n = len(x)\n",
    "    n_c = len(c)\n",
    "    g = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        s = 0.0\n",
    "        for k in range(n_c):\n",
    "            j = k - i\n",
    "            if 0 <= j < n:\n",
    "                s += w[k] * x[j]\n",
    "        g[i] = s\n",
    "    scale = 2.0 * (2.0 * P)\n",
    "    for i in range(n):\n",
    "        g[i] *= scale\n",
    "    return g\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def armijo_step_nb(x, g, P, beta, alpha_init, rho=0.5, c1=1e-4, max_bt=30):\n",
    "    \"\"\"Armijo backtracking line search.\"\"\"\n",
    "    fval = lse_obj_nb(x, P, beta)\n",
    "    alpha = alpha_init\n",
    "    x_new = np.empty_like(x)\n",
    "    for _ in range(max_bt):\n",
    "        for i in range(len(x)):\n",
    "            x_new[i] = x[i] - alpha * g[i]\n",
    "        x_new = project_simplex_nb(x_new)\n",
    "        fval_new = lse_obj_nb(x_new, P, beta)\n",
    "        descent = 0.0\n",
    "        for i in range(len(x)):\n",
    "            descent += g[i] * (x[i] - x_new[i])\n",
    "        if fval_new <= fval - c1 * descent:\n",
    "            return x_new, fval_new, alpha\n",
    "        alpha *= rho\n",
    "    return x_new, fval_new, alpha\n",
    "\n",
    "\n",
    "# === Warm up JIT (first call compiles; subsequent calls are fast) ===\n",
    "_x_warmup = np.ones(5) / 5.0\n",
    "_ = project_simplex_nb(_x_warmup)\n",
    "_ = autoconv_coeffs(_x_warmup, 5)\n",
    "_ = lse_obj_nb(_x_warmup, 5, 10.0)\n",
    "_ = lse_grad_nb(_x_warmup, 5, 10.0)\n",
    "_ = armijo_step_nb(_x_warmup, np.ones(5), 5, 10.0, 0.1)\n",
    "\n",
    "# Verify gradient with finite differences\n",
    "np.random.seed(42)\n",
    "P_test = 10\n",
    "x_test = np.random.dirichlet(np.ones(P_test))\n",
    "beta_test = 10.0\n",
    "g_jit = lse_grad_nb(x_test, P_test, beta_test)\n",
    "eps = 1e-7\n",
    "g_fd = np.zeros(P_test)\n",
    "for i in range(P_test):\n",
    "    x_p = x_test.copy(); x_p[i] += eps\n",
    "    x_m = x_test.copy(); x_m[i] -= eps\n",
    "    g_fd[i] = (lse_obj_nb(x_p, P_test, beta_test) - lse_obj_nb(x_m, P_test, beta_test)) / (2 * eps)\n",
    "print(f\"JIT gradient vs finite diff: max error = {np.max(np.abs(g_jit - g_fd)):.2e}\")\n",
    "print(\"JIT compilation and verification complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fba3d2",
   "metadata": {},
   "source": [
    "## LSE Nesterov Optimizer\n",
    "\n",
    "Single-restart LSE optimizer using Nesterov momentum with Armijo line search,\n",
    "and $\\beta$-continuation through a schedule of increasing smoothness parameters.\n",
    "Wrapped in a parallel multi-restart driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96571fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(cache=True)\n",
    "def _lse_single_restart(x_init, P, beta_schedule, n_iters_per_beta):\n",
    "    \"\"\"Run one full LSE Nesterov continuation from x_init. Returns (best_true_val, best_x).\"\"\"\n",
    "    x = x_init.copy()\n",
    "    best_true_val = 1e300\n",
    "    best_x = x.copy()\n",
    "\n",
    "    for stage in range(len(beta_schedule)):\n",
    "        beta = beta_schedule[stage]\n",
    "        y = x.copy()\n",
    "        x_prev = x.copy()\n",
    "        alpha_init = 0.1\n",
    "        best_stage_val = 1e300\n",
    "        best_stage_x = x.copy()\n",
    "        no_improve = 0\n",
    "\n",
    "        for t in range(n_iters_per_beta):\n",
    "            g = lse_grad_nb(y, P, beta)\n",
    "            x_new, fval_new, alpha_used = armijo_step_nb(y, g, P, beta, alpha_init)\n",
    "            alpha_init = min(alpha_used * 2.0, 1.0)\n",
    "\n",
    "            # Nesterov momentum\n",
    "            momentum = t / (t + 3.0)\n",
    "            n = len(x_new)\n",
    "            y_new = np.empty(n)\n",
    "            for i in range(n):\n",
    "                y_new[i] = x_new[i] + momentum * (x_new[i] - x_prev[i])\n",
    "            y_new = project_simplex_nb(y_new)\n",
    "\n",
    "            x_prev = x_new.copy()\n",
    "            x = x_new\n",
    "            y = y_new\n",
    "\n",
    "            true_val = np.max(autoconv_coeffs(x, P))\n",
    "            if true_val < best_stage_val:\n",
    "                best_stage_val = true_val\n",
    "                best_stage_x = x.copy()\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "\n",
    "            if no_improve > 1000:\n",
    "                break\n",
    "\n",
    "        x = best_stage_x\n",
    "\n",
    "    true_val = np.max(autoconv_coeffs(x, P))\n",
    "    if true_val < best_true_val:\n",
    "        best_true_val = true_val\n",
    "        best_x = x.copy()\n",
    "\n",
    "    return best_true_val, best_x\n",
    "\n",
    "\n",
    "# Warm up the single-restart function\n",
    "_bs = np.array([1.0, 10.0, 100.0])\n",
    "_ = _lse_single_restart(np.ones(5) / 5.0, 5, _bs, 10)\n",
    "\n",
    "\n",
    "def optimize_lse_parallel(P, beta_schedule, n_iters_per_beta=10000,\n",
    "                           n_restarts=30, n_jobs=-1, verbose=True):\n",
    "    \"\"\"Parallel LSE Nesterov: each restart runs on a separate CPU core.\"\"\"\n",
    "    beta_arr = np.array(beta_schedule, dtype=np.float64)\n",
    "\n",
    "    # Pre-generate all random initializations (Dirichlet)\n",
    "    rng = np.random.default_rng()\n",
    "    inits = [rng.dirichlet(np.ones(P)) for _ in range(n_restarts)]\n",
    "\n",
    "    # Run restarts in parallel across cores\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=0)(\n",
    "        delayed(_lse_single_restart)(inits[i], P, beta_arr, n_iters_per_beta)\n",
    "        for i in range(n_restarts)\n",
    "    )\n",
    "\n",
    "    # Find best across all restarts\n",
    "    best_val = np.inf\n",
    "    best_x = None\n",
    "    for i, (val, x) in enumerate(results):\n",
    "        if verbose and (i % 10 == 0 or val < best_val):\n",
    "            tag = \"  <-- best\" if (best_x is None or val < best_val) else \"\"\n",
    "            print(f\"    Restart {i:>3}: {val:.6f}{tag}\")\n",
    "        if val < best_val:\n",
    "            best_val = val\n",
    "            best_x = x.copy()\n",
    "\n",
    "    return best_val, best_x\n",
    "\n",
    "\n",
    "print(\"Parallel LSE Nesterov optimizer defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2d7a67",
   "metadata": {},
   "source": [
    "## Polyak Subgradient Optimizer\n",
    "\n",
    "Polyak step-size subgradient method for the non-smooth $\\max_k c_k$ objective.\n",
    "Uses a target value to compute step sizes. Wrapped in a parallel multi-restart,\n",
    "multi-target driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5facecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(cache=True)\n",
    "def _polyak_single_restart(x_init, P, target, n_iters):\n",
    "    \"\"\"Run one Polyak subgradient trajectory. Returns (best_val, best_x).\"\"\"\n",
    "    x = x_init.copy()\n",
    "    n = len(x)\n",
    "    best_val = 1e300\n",
    "    best_x = x.copy()\n",
    "\n",
    "    for t in range(n_iters):\n",
    "        c = autoconv_coeffs(x, P)\n",
    "        fval = np.max(c)\n",
    "        if fval < best_val:\n",
    "            best_val = fval\n",
    "            best_x = x.copy()\n",
    "\n",
    "        # Find argmax\n",
    "        k_star = 0\n",
    "        for k in range(len(c)):\n",
    "            if c[k] > c[k_star]:\n",
    "                k_star = k\n",
    "\n",
    "        # Subgradient\n",
    "        g = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            j = k_star - i\n",
    "            if 0 <= j < n:\n",
    "                g[i] = 2.0 * (2.0 * P) * x[j]\n",
    "\n",
    "        gnorm2 = 0.0\n",
    "        for i in range(n):\n",
    "            gnorm2 += g[i] * g[i]\n",
    "        if gnorm2 < 1e-20:\n",
    "            break\n",
    "\n",
    "        step = (fval - target) / gnorm2\n",
    "        if step < 0.0:\n",
    "            step = 1e-4 / (1.0 + t)\n",
    "\n",
    "        for i in range(n):\n",
    "            x[i] = x[i] - step * g[i]\n",
    "        x = project_simplex_nb(x)\n",
    "\n",
    "    return best_val, best_x\n",
    "\n",
    "\n",
    "# Warm up\n",
    "_ = _polyak_single_restart(np.ones(5) / 5.0, 5, 1.5, 10)\n",
    "\n",
    "\n",
    "def polyak_parallel(P, n_iters=100000, n_restarts=30,\n",
    "                    targets=(1.51, 1.505, 1.50, 1.495, 1.49, 1.48),\n",
    "                    n_jobs=-1, verbose=True):\n",
    "    \"\"\"Parallel Polyak subgradient across all (target, restart) pairs.\"\"\"\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # Build all (init, target) pairs\n",
    "    tasks = []\n",
    "    for target in targets:\n",
    "        for _ in range(n_restarts):\n",
    "            tasks.append((rng.dirichlet(np.ones(P)), target))\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=0)(\n",
    "        delayed(_polyak_single_restart)(init, P, target, n_iters)\n",
    "        for init, target in tasks\n",
    "    )\n",
    "\n",
    "    best_val = np.inf\n",
    "    best_x = None\n",
    "    for val, x in results:\n",
    "        if val < best_val:\n",
    "            best_val = val\n",
    "            best_x = x.copy()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"    Best across {len(tasks)} runs: {best_val:.6f}\")\n",
    "\n",
    "    return best_val, best_x\n",
    "\n",
    "\n",
    "print(\"Parallel Polyak optimizer defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c18dc9",
   "metadata": {},
   "source": [
    "## LSE vs Polyak: Head-to-Head Comparison\n",
    "\n",
    "Run both optimizers across multiple P values (10 to 200) with 30 restarts each,\n",
    "then compare exact peak autoconvolution values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad6a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_values = [10, 20, 30, 50, 75, 100, 150, 200]\n",
    "beta_schedule = [1, 2, 4, 8, 15, 30, 60, 100, 150, 250, 400, 600, 1000, 1500, 2000]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for P in P_values:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"P = {P}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    edges = np.linspace(-0.25, 0.25, P + 1)\n",
    "    bin_width = 0.5 / P\n",
    "\n",
    "    # LSE Nesterov (30 restarts, parallel)\n",
    "    t0 = time.time()\n",
    "    print(f\"  LSE Nesterov (30 restarts, 15 stages, 10k iters/stage):\")\n",
    "    val_lse, x_lse = optimize_lse_parallel(\n",
    "        P, beta_schedule, n_iters_per_beta=10000, n_restarts=30\n",
    "    )\n",
    "    exact_lse = exact_val(x_lse, P)\n",
    "    dt_lse = time.time() - t0\n",
    "    print(f\"  => LSE exact = {exact_lse:.6f}  ({dt_lse:.1f}s)\")\n",
    "\n",
    "    # Polyak (180 runs, parallel)\n",
    "    t0 = time.time()\n",
    "    print(f\"  Polyak (180 runs, 100k iters):\")\n",
    "    val_poly, x_poly = polyak_parallel(P, n_iters=100000, n_restarts=30)\n",
    "    exact_poly = exact_val(x_poly, P)\n",
    "    dt_poly = time.time() - t0\n",
    "    print(f\"  => Polyak exact = {exact_poly:.6f}  ({dt_poly:.1f}s)\")\n",
    "\n",
    "    delta = exact_lse - exact_poly\n",
    "    winner = \"LSE\" if delta < 0 else \"Polyak\" if delta > 0 else \"Tie\"\n",
    "    print(f\"  >> Winner: {winner} (delta = {delta:+.6f})\")\n",
    "\n",
    "    results[P] = {\n",
    "        'lse': (exact_lse, x_lse),\n",
    "        'polyak': (exact_poly, x_poly),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Results table ===\n",
    "\n",
    "print(f\"{'P':>5} | {'LSE (exact)':>14} | {'Polyak (exact)':>14} | {'Delta':>10} | {'Winner':>8}\")\n",
    "print('-' * 62)\n",
    "for P in P_values:\n",
    "    r = results[P]\n",
    "    v_lse = r['lse'][0]\n",
    "    v_poly = r['polyak'][0]\n",
    "    delta = v_lse - v_poly\n",
    "    winner = \"LSE\" if delta < 0 else \"Polyak\" if delta > 0 else \"Tie\"\n",
    "    print(f\"{P:>5} | {v_lse:>14.6f} | {v_poly:>14.6f} | {delta:>+10.6f} | {winner:>8}\")\n",
    "\n",
    "print(f\"\\nBest known upper bound: 1.5029\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5857c7",
   "metadata": {},
   "source": [
    "## Visualization: LSE vs Polyak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, P in enumerate(P_values):\n",
    "    ax = axes[idx]\n",
    "    edges = np.linspace(-0.25, 0.25, P + 1)\n",
    "    centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "    bin_width = 0.5 / P\n",
    "\n",
    "    x_lse = results[P]['lse'][1]\n",
    "    x_poly = results[P]['polyak'][1]\n",
    "\n",
    "    ax.bar(centers - bin_width*0.15, x_lse / bin_width, width=bin_width*0.35,\n",
    "           alpha=0.7, label='LSE', color='C0', edgecolor='k', linewidth=0.3)\n",
    "    ax.bar(centers + bin_width*0.15, x_poly / bin_width, width=bin_width*0.35,\n",
    "           alpha=0.7, label='Polyak', color='C1', edgecolor='k', linewidth=0.3)\n",
    "\n",
    "    v_lse = results[P]['lse'][0]\n",
    "    v_poly = results[P]['polyak'][0]\n",
    "    ax.set_title(f'P={P}\\nLSE={v_lse:.4f}, Poly={v_poly:.4f}', fontsize=9)\n",
    "    if idx == 0:\n",
    "        ax.legend(fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('LSE vs Polyak: step function heights', y=1.02, fontsize=14)\n",
    "plt.savefig('logsumexp_vs_polyak_heights.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Convergence comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "Ps = sorted(results.keys())\n",
    "lse_vals = [results[P]['lse'][0] for P in Ps]\n",
    "poly_vals = [results[P]['polyak'][0] for P in Ps]\n",
    "\n",
    "ax.plot(Ps, lse_vals, 'o-', label='LSE continuation', markersize=8)\n",
    "ax.plot(Ps, poly_vals, 's--', label='Polyak subgradient', markersize=8)\n",
    "ax.axhline(1.5029, color='r', linestyle='--', alpha=0.5, label='Best known (1.5029)')\n",
    "ax.set_xlabel('P (number of bins)')\n",
    "ax.set_ylabel('Peak autoconvolution (exact)')\n",
    "ax.set_title('LSE continuation vs Polyak subgradient')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('logsumexp_vs_polyak_convergence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53cd741",
   "metadata": {},
   "source": [
    "## Ablation: Beta Schedule\n",
    "\n",
    "Test different $\\beta$-continuation schedules at P=50 to understand the sensitivity\n",
    "of LSE continuation to the schedule choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a15ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_abl = 50\n",
    "schedules = {\n",
    "    'aggressive': np.array([1., 10., 100., 1000.]),\n",
    "    'moderate': np.array([1., 2., 5., 10., 20., 50., 100., 200., 500., 1000.]),\n",
    "    'fine_15': np.array([1., 2., 4., 8., 15., 30., 60., 100., 150., 250., 400., 600., 1000., 1500., 2000.]),\n",
    "    'gentle': np.array([1., 1.5, 2., 3., 5., 7., 10., 15., 20., 30., 50., 75., 100., 150., 200., 300., 500., 750., 1000.]),\n",
    "    'fixed_low': np.full(10, 10.0),\n",
    "    'fixed_high': np.full(10, 1000.0),\n",
    "}\n",
    "\n",
    "print(f\"Beta schedule ablation at P = {P_abl} (20 restarts each, parallel)\")\n",
    "print(f\"{'Schedule':>15} | {'Stages':>6} | {'Exact obj':>12}\")\n",
    "print('-' * 42)\n",
    "\n",
    "for name, sched in schedules.items():\n",
    "    val, x = optimize_lse_parallel(\n",
    "        P_abl, sched, n_iters_per_beta=8000, n_restarts=20, verbose=False\n",
    "    )\n",
    "    exact = exact_val(x, P_abl)\n",
    "    print(f\"{name:>15} | {len(sched):>6} | {exact:>12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72b384",
   "metadata": {},
   "source": [
    "## Hybrid Optimizer: LSE Continuation + Adaptive Polyak Polish\n",
    "\n",
    "Two-phase approach:\n",
    "1. **Phase 1 (LSE):** Find a good basin via smooth $\\beta$-continuation with Nesterov acceleration\n",
    "2. **Phase 2 (Polyak):** Polish with adaptive Polyak subgradient using a shrinking target offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b377f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(cache=True)\n",
    "def _polyak_polish_nb(x_init, P, n_iters):\n",
    "    \"\"\"Adaptive Polyak polish: target = best_val - shrinking_offset.\"\"\"\n",
    "    x = x_init.copy()\n",
    "    n = len(x)\n",
    "    best_val = np.max(autoconv_coeffs(x, P))\n",
    "    best_x = x.copy()\n",
    "\n",
    "    for t in range(n_iters):\n",
    "        c = autoconv_coeffs(x, P)\n",
    "        fval = np.max(c)\n",
    "        if fval < best_val:\n",
    "            best_val = fval\n",
    "            best_x = x.copy()\n",
    "\n",
    "        offset = 0.01 / (1.0 + t * 1e-4)\n",
    "        target = best_val - offset\n",
    "\n",
    "        k_star = 0\n",
    "        for k in range(len(c)):\n",
    "            if c[k] > c[k_star]:\n",
    "                k_star = k\n",
    "\n",
    "        g = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            j = k_star - i\n",
    "            if 0 <= j < n:\n",
    "                g[i] = 2.0 * (2.0 * P) * x[j]\n",
    "\n",
    "        gnorm2 = 0.0\n",
    "        for i in range(n):\n",
    "            gnorm2 += g[i] * g[i]\n",
    "        if gnorm2 < 1e-20:\n",
    "            break\n",
    "\n",
    "        step = (fval - target) / gnorm2\n",
    "        if step < 0.0:\n",
    "            step = 1e-5 / (1.0 + t * 1e-4)\n",
    "\n",
    "        for i in range(n):\n",
    "            x[i] = x[i] - step * g[i]\n",
    "        x = project_simplex_nb(x)\n",
    "\n",
    "    return best_val, best_x\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def _hybrid_single_restart(x_init, P, beta_schedule, n_iters_lse, n_iters_polyak):\n",
    "    \"\"\"One restart: LSE Nesterov continuation -> adaptive Polyak polish.\"\"\"\n",
    "    x = x_init.copy()\n",
    "\n",
    "    # Phase 1: LSE continuation\n",
    "    for stage in range(len(beta_schedule)):\n",
    "        beta = beta_schedule[stage]\n",
    "        y = x.copy()\n",
    "        x_prev = x.copy()\n",
    "        alpha_init = 0.1\n",
    "        best_stage_val = 1e300\n",
    "        best_stage_x = x.copy()\n",
    "        no_improve = 0\n",
    "\n",
    "        for t in range(n_iters_lse):\n",
    "            g = lse_grad_nb(y, P, beta)\n",
    "            x_new, fval_new, alpha_used = armijo_step_nb(y, g, P, beta, alpha_init)\n",
    "            alpha_init = min(alpha_used * 2.0, 1.0)\n",
    "\n",
    "            momentum = t / (t + 3.0)\n",
    "            n = len(x_new)\n",
    "            y_new = np.empty(n)\n",
    "            for i in range(n):\n",
    "                y_new[i] = x_new[i] + momentum * (x_new[i] - x_prev[i])\n",
    "            y_new = project_simplex_nb(y_new)\n",
    "\n",
    "            x_prev = x_new.copy()\n",
    "            x = x_new\n",
    "            y = y_new\n",
    "\n",
    "            tv = np.max(autoconv_coeffs(x, P))\n",
    "            if tv < best_stage_val:\n",
    "                best_stage_val = tv\n",
    "                best_stage_x = x.copy()\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "            if no_improve > 800:\n",
    "                break\n",
    "\n",
    "        x = best_stage_x\n",
    "\n",
    "    lse_val = np.max(autoconv_coeffs(x, P))\n",
    "\n",
    "    # Phase 2: adaptive Polyak polish\n",
    "    polished_val, polished_x = _polyak_polish_nb(x, P, n_iters_polyak)\n",
    "\n",
    "    return lse_val, polished_val, polished_x\n",
    "\n",
    "\n",
    "# Warm up\n",
    "_bs = np.array([1.0, 10.0])\n",
    "_ = _polyak_polish_nb(np.ones(5) / 5.0, 5, 10)\n",
    "_ = _hybrid_single_restart(np.ones(5) / 5.0, 5, _bs, 10, 10)\n",
    "\n",
    "\n",
    "def hybrid_parallel(P, beta_schedule, n_iters_lse=10000,\n",
    "                     n_iters_polyak=100000, n_restarts=30, n_jobs=-1, verbose=True):\n",
    "    \"\"\"Parallel hybrid: LSE basin-finding + adaptive Polyak polishing.\"\"\"\n",
    "    beta_arr = np.array(beta_schedule, dtype=np.float64)\n",
    "    rng = np.random.default_rng()\n",
    "    inits = [rng.dirichlet(np.ones(P)) for _ in range(n_restarts)]\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=0)(\n",
    "        delayed(_hybrid_single_restart)(inits[i], P, beta_arr, n_iters_lse, n_iters_polyak)\n",
    "        for i in range(n_restarts)\n",
    "    )\n",
    "\n",
    "    best_val = np.inf\n",
    "    best_x = None\n",
    "    for i, (lse_v, pol_v, x) in enumerate(results):\n",
    "        if verbose and (i % 10 == 0 or pol_v < best_val):\n",
    "            tag = \"  <-- best\" if (best_x is None or pol_v < best_val) else \"\"\n",
    "            print(f\"    Restart {i:>3}: LSE={lse_v:.6f} -> polished={pol_v:.6f}{tag}\")\n",
    "        if pol_v < best_val:\n",
    "            best_val = pol_v\n",
    "            best_x = x.copy()\n",
    "\n",
    "    return best_val, best_x\n",
    "\n",
    "\n",
    "print(\"Parallel hybrid optimizer defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d262e907",
   "metadata": {},
   "source": [
    "## Three-Method Head-to-Head: LSE vs Polyak vs Hybrid\n",
    "\n",
    "Run all three methods at P = 20, 50, 100, 200 and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd74a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_values_hybrid = [20, 50, 100, 200]\n",
    "beta_sched = [1, 2, 4, 8, 15, 30, 60, 100, 150, 250, 400, 600, 1000, 1500, 2000]\n",
    "\n",
    "results_hybrid = {}\n",
    "\n",
    "for P in P_values_hybrid:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"P = {P}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    edges = np.linspace(-0.25, 0.25, P + 1)\n",
    "    bin_width = 0.5 / P\n",
    "\n",
    "    # Hybrid (30 restarts, LSE -> adaptive Polyak, parallel)\n",
    "    t0 = time.time()\n",
    "    print(f\"  Hybrid (30 restarts, parallel):\")\n",
    "    val_hyb, x_hyb = hybrid_parallel(\n",
    "        P, beta_sched, n_iters_lse=10000, n_iters_polyak=100000, n_restarts=30\n",
    "    )\n",
    "    exact_hyb = exact_val(x_hyb, P)\n",
    "    dt = time.time() - t0\n",
    "    print(f\"  => Hybrid exact = {exact_hyb:.6f}  ({dt:.1f}s)\")\n",
    "\n",
    "    exact_lse = results.get(P, {}).get('lse', (None,))[0]\n",
    "    exact_poly = results.get(P, {}).get('polyak', (None,))[0]\n",
    "\n",
    "    results_hybrid[P] = {\n",
    "        'hybrid': exact_hyb,\n",
    "        'lse_only': exact_lse,\n",
    "        'polyak_only': exact_poly,\n",
    "    }\n",
    "\n",
    "# === Final comparison table ===\n",
    "print(f\"\\n\\n{'='*72}\")\n",
    "print(f\"FINAL RESULTS\")\n",
    "print(f\"{'='*72}\")\n",
    "print(f\"{'P':>5} | {'Hybrid':>12} | {'LSE only':>12} | {'Polyak only':>12} | {'Best known':>12}\")\n",
    "print('-' * 68)\n",
    "for P in P_values_hybrid:\n",
    "    r = results_hybrid[P]\n",
    "    hyb_s = f\"{r['hybrid']:>12.6f}\" if r['hybrid'] is not None else f\"{'N/A':>12}\"\n",
    "    lse_s = f\"{r['lse_only']:>12.6f}\" if r['lse_only'] is not None else f\"{'N/A':>12}\"\n",
    "    pol_s = f\"{r['polyak_only']:>12.6f}\" if r['polyak_only'] is not None else f\"{'N/A':>12}\"\n",
    "    print(f\"{P:>5} | {hyb_s} | {lse_s} | {pol_s} | {1.5029:>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5680f7",
   "metadata": {},
   "source": [
    "## Save Initial Best Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433062d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect best solution per P across all methods\n",
    "all_best = {}\n",
    "\n",
    "# From LSE-only and Polyak-only\n",
    "for P in P_values:\n",
    "    edges = np.linspace(-0.25, 0.25, P + 1)\n",
    "    bin_width = 0.5 / P\n",
    "    for method_name, key in [('lse', 'lse'), ('polyak', 'polyak')]:\n",
    "        exact_v, x_simplex = results[P][key]\n",
    "        entry = {\n",
    "            'method': method_name,\n",
    "            'P': P,\n",
    "            'claimed_exact_peak': exact_v,\n",
    "            'simplex_weights': x_simplex.tolist(),\n",
    "            'edges': edges.tolist(),\n",
    "            'heights': (x_simplex / bin_width).tolist(),\n",
    "        }\n",
    "        label = f\"{method_name}_P{P}\"\n",
    "        all_best[label] = entry\n",
    "\n",
    "# From hybrid -- re-run to get x vectors (they were not stored in results_hybrid)\n",
    "print(\"Re-running hybrid at key P values to save solutions...\")\n",
    "beta_sched_save = [1, 2, 4, 8, 15, 30, 60, 100, 150, 250, 400, 600, 1000, 1500, 2000]\n",
    "for P in P_values_hybrid:\n",
    "    edges = np.linspace(-0.25, 0.25, P + 1)\n",
    "    bin_width = 0.5 / P\n",
    "    val_hyb, x_hyb = hybrid_parallel(\n",
    "        P, beta_sched_save, n_iters_lse=10000, n_iters_polyak=100000,\n",
    "        n_restarts=30, verbose=False\n",
    "    )\n",
    "    exact_hyb = exact_val(x_hyb, P)\n",
    "    entry = {\n",
    "        'method': 'hybrid',\n",
    "        'P': P,\n",
    "        'claimed_exact_peak': exact_hyb,\n",
    "        'simplex_weights': x_hyb.tolist(),\n",
    "        'edges': edges.tolist(),\n",
    "        'heights': (x_hyb / bin_width).tolist(),\n",
    "    }\n",
    "    all_best[f\"hybrid_P{P}\"] = entry\n",
    "    print(f\"  hybrid P={P}: {exact_hyb:.6f}\")\n",
    "\n",
    "# Find the overall best solution\n",
    "best_label = min(all_best, key=lambda k: all_best[k]['claimed_exact_peak'])\n",
    "best_entry = all_best[best_label]\n",
    "print(f\"\\nOverall best: {best_label} -> {best_entry['claimed_exact_peak']:.6f}\")\n",
    "\n",
    "# Save everything\n",
    "out_path = os.path.join('.', 'best_solutions.json')\n",
    "with open(out_path, 'w') as f:\n",
    "    json.dump(all_best, f, indent=2)\n",
    "print(f\"Saved {len(all_best)} solutions to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634d2f28",
   "metadata": {},
   "source": [
    "# Heavy Compute: Multi-Strategy Hybrid Search (~2-3 hours)\n",
    "\n",
    "We test **12 different initialization strategies** for the hybrid (LSE continuation\n",
    "-> Polyak polish) optimizer, across multiple P values, with many restarts and\n",
    "extended iterations.\n",
    "\n",
    "## Initialization Strategies\n",
    "1. **dirichlet_uniform** -- Dirichlet(alpha=1) (current baseline)\n",
    "2. **dirichlet_sparse** -- Dirichlet(alpha=0.1), favors sparse solutions\n",
    "3. **dirichlet_concentrated** -- Dirichlet(alpha=5), near-uniform\n",
    "4. **gaussian_peak** -- Gaussian centered at 0, random width\n",
    "5. **bimodal** -- Two Gaussians at +/-offset\n",
    "6. **cosine_shaped** -- cos(pi*x/0.25) profile\n",
    "7. **triangle** -- Triangle centered at 0\n",
    "8. **flat_noisy** -- Uniform + small perturbation\n",
    "9. **boundary_heavy** -- Extra weight near +/-0.25 edges\n",
    "10. **random_sparse_k** -- Only k random bins nonzero\n",
    "11. **symmetric_dirichlet** -- Enforce f(x)=f(-x) symmetry\n",
    "12. **warm_perturb** -- Best known solution + Gaussian noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c431b2",
   "metadata": {},
   "source": [
    "## Initialization Strategy Generators and Extended Hybrid Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f9639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inits(strategy, P, n_restarts, rng, warm_x=None):\n",
    "    \"\"\"Generate n_restarts initial simplex vectors for a given strategy.\"\"\"\n",
    "    centers = np.linspace(-0.25 + 0.25/P, 0.25 - 0.25/P, P)\n",
    "    inits = []\n",
    "\n",
    "    for _ in range(n_restarts):\n",
    "        if strategy == 'dirichlet_uniform':\n",
    "            x = rng.dirichlet(np.ones(P))\n",
    "\n",
    "        elif strategy == 'dirichlet_sparse':\n",
    "            x = rng.dirichlet(np.full(P, 0.1))\n",
    "\n",
    "        elif strategy == 'dirichlet_concentrated':\n",
    "            x = rng.dirichlet(np.full(P, 5.0))\n",
    "\n",
    "        elif strategy == 'gaussian_peak':\n",
    "            sigma = rng.uniform(0.03, 0.15)\n",
    "            mu = rng.uniform(-0.05, 0.05)\n",
    "            x = np.exp(-0.5 * ((centers - mu) / sigma) ** 2)\n",
    "            x += rng.uniform(0, 0.01, P)  # small noise floor\n",
    "            x /= x.sum()\n",
    "\n",
    "        elif strategy == 'bimodal':\n",
    "            sep = rng.uniform(0.05, 0.2)\n",
    "            sigma = rng.uniform(0.02, 0.08)\n",
    "            ratio = rng.uniform(0.3, 0.7)\n",
    "            x = ratio * np.exp(-0.5 * ((centers - sep/2) / sigma) ** 2)\n",
    "            x += (1 - ratio) * np.exp(-0.5 * ((centers + sep/2) / sigma) ** 2)\n",
    "            x += rng.uniform(0, 0.005, P)\n",
    "            x /= x.sum()\n",
    "\n",
    "        elif strategy == 'cosine_shaped':\n",
    "            phase = rng.uniform(0, np.pi)\n",
    "            freq = rng.uniform(0.5, 3.0)\n",
    "            x = np.cos(freq * np.pi * centers / 0.25 + phase) ** 2\n",
    "            x += rng.uniform(0, 0.02, P)\n",
    "            x /= x.sum()\n",
    "\n",
    "        elif strategy == 'triangle':\n",
    "            peak_pos = rng.uniform(-0.1, 0.1)\n",
    "            width = rng.uniform(0.1, 0.25)\n",
    "            x = np.maximum(0, 1 - np.abs(centers - peak_pos) / width)\n",
    "            x += rng.uniform(0, 0.01, P)\n",
    "            x /= x.sum()\n",
    "\n",
    "        elif strategy == 'flat_noisy':\n",
    "            noise_scale = rng.uniform(0.01, 0.2)\n",
    "            x = np.ones(P) + noise_scale * rng.standard_normal(P)\n",
    "            x = np.maximum(x, 0.0)\n",
    "            x /= x.sum()\n",
    "\n",
    "        elif strategy == 'boundary_heavy':\n",
    "            # Extra mass near edges of [-0.25, 0.25]\n",
    "            decay = rng.uniform(2.0, 10.0)\n",
    "            x = np.exp(-decay * np.abs(np.abs(centers) - 0.25))\n",
    "            x += rng.uniform(0, 0.01, P)\n",
    "            x /= x.sum()\n",
    "\n",
    "        elif strategy == 'random_sparse_k':\n",
    "            k = rng.integers(max(3, P // 10), max(4, P // 3))\n",
    "            x = np.zeros(P)\n",
    "            idx = rng.choice(P, size=k, replace=False)\n",
    "            x[idx] = rng.dirichlet(np.ones(k))\n",
    "\n",
    "        elif strategy == 'symmetric_dirichlet':\n",
    "            half = P // 2\n",
    "            x_half = rng.dirichlet(np.ones(half))\n",
    "            x = np.zeros(P)\n",
    "            x[:half] = x_half\n",
    "            x[P - half:] = x_half[::-1]\n",
    "            if P % 2 == 1:\n",
    "                x[half] = rng.uniform(0.0, 0.1)\n",
    "            x /= x.sum()\n",
    "\n",
    "        elif strategy == 'warm_perturb':\n",
    "            if warm_x is not None:\n",
    "                noise = rng.uniform(0.3, 1.5)\n",
    "                x = warm_x.copy() + noise * rng.standard_normal(P) * np.mean(warm_x)\n",
    "                x = np.maximum(x, 0.0)\n",
    "                if x.sum() < 1e-12:\n",
    "                    x = rng.dirichlet(np.ones(P))\n",
    "                else:\n",
    "                    x /= x.sum()\n",
    "            else:\n",
    "                x = rng.dirichlet(np.ones(P))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "        inits.append(x)\n",
    "\n",
    "    return inits\n",
    "\n",
    "\n",
    "def hybrid_strategy_run(P, strategy, beta_schedule, n_iters_lse=15000,\n",
    "                        n_iters_polyak=200000, n_restarts=80,\n",
    "                        n_jobs=-1, warm_x=None, verbose=True):\n",
    "    \"\"\"Run hybrid optimizer with a specific initialization strategy.\"\"\"\n",
    "    beta_arr = np.array(beta_schedule, dtype=np.float64)\n",
    "    rng = np.random.default_rng()\n",
    "    inits = make_inits(strategy, P, n_restarts, rng, warm_x=warm_x)\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=0)(\n",
    "        delayed(_hybrid_single_restart)(inits[i], P, beta_arr, n_iters_lse, n_iters_polyak)\n",
    "        for i in range(n_restarts)\n",
    "    )\n",
    "\n",
    "    best_val = np.inf\n",
    "    best_x = None\n",
    "    all_vals = []\n",
    "    for i, (lse_v, pol_v, x) in enumerate(results):\n",
    "        all_vals.append(pol_v)\n",
    "        if pol_v < best_val:\n",
    "            best_val = pol_v\n",
    "            best_x = x.copy()\n",
    "\n",
    "    if verbose:\n",
    "        arr = np.array(all_vals)\n",
    "        print(f\"    {strategy:30s}  best={best_val:.6f}  \"\n",
    "              f\"median={np.median(arr):.6f}  std={np.std(arr):.6f}\")\n",
    "\n",
    "    return best_val, best_x, all_vals\n",
    "\n",
    "\n",
    "def upsample_solution(x_low, P_low, P_high):\n",
    "    \"\"\"Upsample a P_low-bin solution to P_high bins via linear interpolation.\"\"\"\n",
    "    edges_low = np.linspace(-0.25, 0.25, P_low + 1)\n",
    "    edges_high = np.linspace(-0.25, 0.25, P_high + 1)\n",
    "    bin_width_low = 0.5 / P_low\n",
    "    bin_width_high = 0.5 / P_high\n",
    "\n",
    "    # Convert to density, interpolate, convert back to simplex weights\n",
    "    heights_low = x_low / bin_width_low\n",
    "    centers_low = 0.5 * (edges_low[:-1] + edges_low[1:])\n",
    "    centers_high = 0.5 * (edges_high[:-1] + edges_high[1:])\n",
    "\n",
    "    heights_high = np.interp(centers_high, centers_low, heights_low)\n",
    "    heights_high = np.maximum(heights_high, 0.0)\n",
    "\n",
    "    x_high = heights_high * bin_width_high\n",
    "    if x_high.sum() > 0:\n",
    "        x_high /= x_high.sum()\n",
    "    else:\n",
    "        x_high = np.ones(P_high) / P_high\n",
    "    return x_high\n",
    "\n",
    "\n",
    "# Quick validation of initialization strategies\n",
    "rng_test = np.random.default_rng(0)\n",
    "strategies = [\n",
    "    'dirichlet_uniform', 'dirichlet_sparse', 'dirichlet_concentrated',\n",
    "    'gaussian_peak', 'bimodal', 'cosine_shaped', 'triangle',\n",
    "    'flat_noisy', 'boundary_heavy', 'random_sparse_k',\n",
    "    'symmetric_dirichlet', 'warm_perturb',\n",
    "]\n",
    "for s in strategies:\n",
    "    inits = make_inits(s, 50, 3, rng_test, warm_x=np.ones(50)/50)\n",
    "    for x in inits:\n",
    "        assert np.allclose(x.sum(), 1.0, atol=1e-10), f\"{s}: sum={x.sum()}\"\n",
    "        assert np.all(x >= -1e-15), f\"{s}: has negatives\"\n",
    "\n",
    "print(f\"All {len(strategies)} initialization strategies verified.\")\n",
    "print(\"Extended hybrid runner and multi-scale upsampler defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9c9f5",
   "metadata": {},
   "source": [
    "## Round 1: Strategy Sweep at P=200\n",
    "\n",
    "Test all 12 initialization strategies with 80 restarts each to identify the\n",
    "most promising strategies for deeper compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeca1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended beta schedule with finer stages\n",
    "beta_heavy = [1, 1.5, 2, 3, 5, 8, 12, 18, 28, 42, 65, 100, 150, 230, 350,\n",
    "              500, 750, 1000, 1500, 2000, 3000]\n",
    "\n",
    "strategies_all = [\n",
    "    'dirichlet_uniform', 'dirichlet_sparse', 'dirichlet_concentrated',\n",
    "    'gaussian_peak', 'bimodal', 'cosine_shaped', 'triangle',\n",
    "    'flat_noisy', 'boundary_heavy', 'random_sparse_k',\n",
    "    'symmetric_dirichlet', 'warm_perturb',\n",
    "]\n",
    "\n",
    "# Global tracker\n",
    "global_best = {'val': np.inf, 'x': None, 'P': None, 'strategy': None, 'round': None}\n",
    "round_results = {}\n",
    "\n",
    "def update_global_best(val, x, P, strategy, round_name):\n",
    "    if val < global_best['val']:\n",
    "        global_best['val'] = val\n",
    "        global_best['x'] = x.copy()\n",
    "        global_best['P'] = P\n",
    "        global_best['strategy'] = strategy\n",
    "        global_best['round'] = round_name\n",
    "        print(f\"  *** NEW GLOBAL BEST: {val:.6f} (P={P}, {strategy}, {round_name}) ***\")\n",
    "\n",
    "t_total_start = time.time()\n",
    "\n",
    "# --- Round 1: P=200, all strategies, 80 restarts each ---\n",
    "P = 200\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ROUND 1: Strategy sweep at P={P}  (80 restarts x 12 strategies)\")\n",
    "print(f\"{'='*70}\")\n",
    "t0 = time.time()\n",
    "\n",
    "r1_results = {}\n",
    "for strat in strategies_all:\n",
    "    val, x, vals = hybrid_strategy_run(\n",
    "        P, strat, beta_heavy,\n",
    "        n_iters_lse=15000, n_iters_polyak=200000,\n",
    "        n_restarts=80, warm_x=global_best.get('x')\n",
    "    )\n",
    "    ev = exact_val(x, P)\n",
    "    r1_results[strat] = {'val': ev, 'x': x, 'all_vals': vals}\n",
    "    update_global_best(ev, x, P, strat, 'round1_P200')\n",
    "\n",
    "# Rank strategies\n",
    "ranked = sorted(r1_results.items(), key=lambda kv: kv[1]['val'])\n",
    "print(f\"\\nRound 1 ranking (P={P}):\")\n",
    "for i, (s, r) in enumerate(ranked):\n",
    "    print(f\"  {i+1:>2}. {s:30s}  {r['val']:.6f}\")\n",
    "top_strategies = [s for s, _ in ranked[:6]]  # top 6\n",
    "\n",
    "dt = time.time() - t0\n",
    "print(f\"\\nRound 1 completed in {dt:.0f}s ({dt/60:.1f} min)\")\n",
    "round_results['round1'] = {s: r1_results[s]['val'] for s in strategies_all}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675685ce",
   "metadata": {},
   "source": [
    "## Round 2: Top Strategies at P=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 300\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ROUND 2: Top strategies at P={P}  (100 restarts x {len(top_strategies)} strategies)\")\n",
    "print(f\"{'='*70}\")\n",
    "t0 = time.time()\n",
    "\n",
    "# Upsample best known to P=300 for warm starts\n",
    "warm_300 = upsample_solution(global_best['x'], global_best['P'], P) if global_best['x'] is not None else None\n",
    "\n",
    "r2_results = {}\n",
    "for strat in top_strategies:\n",
    "    val, x, vals = hybrid_strategy_run(\n",
    "        P, strat, beta_heavy,\n",
    "        n_iters_lse=15000, n_iters_polyak=300000,\n",
    "        n_restarts=100, warm_x=warm_300\n",
    "    )\n",
    "    ev = exact_val(x, P)\n",
    "    r2_results[strat] = {'val': ev, 'x': x}\n",
    "    update_global_best(ev, x, P, strat, 'round2_P300')\n",
    "\n",
    "# Also run warm_perturb with the current best\n",
    "val, x, vals = hybrid_strategy_run(\n",
    "    P, 'warm_perturb', beta_heavy,\n",
    "    n_iters_lse=15000, n_iters_polyak=300000,\n",
    "    n_restarts=100, warm_x=warm_300\n",
    ")\n",
    "ev = exact_val(x, P)\n",
    "r2_results['warm_perturb'] = {'val': ev, 'x': x}\n",
    "update_global_best(ev, x, P, 'warm_perturb', 'round2_P300')\n",
    "\n",
    "dt = time.time() - t0\n",
    "print(f\"\\nRound 2 completed in {dt:.0f}s ({dt/60:.1f} min)\")\n",
    "print(f\"Global best so far: {global_best['val']:.6f} (P={global_best['P']}, {global_best['strategy']})\")\n",
    "\n",
    "# Update top strategies based on round 2\n",
    "ranked_r2 = sorted(r2_results.items(), key=lambda kv: kv[1]['val'])\n",
    "top_strategies_r2 = [s for s, _ in ranked_r2[:4]]\n",
    "print(f\"Top strategies after round 2: {top_strategies_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e675f76",
   "metadata": {},
   "source": [
    "## Round 3: Top Strategies at P=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec64987",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 500\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ROUND 3: Top strategies at P={P}  (80 restarts x {len(top_strategies_r2)} strategies + warm_perturb)\")\n",
    "print(f\"{'='*70}\")\n",
    "t0 = time.time()\n",
    "\n",
    "warm_500 = upsample_solution(global_best['x'], global_best['P'], P) if global_best['x'] is not None else None\n",
    "\n",
    "r3_results = {}\n",
    "strats_r3 = list(set(top_strategies_r2 + ['warm_perturb', 'dirichlet_uniform']))\n",
    "for strat in strats_r3:\n",
    "    val, x, vals = hybrid_strategy_run(\n",
    "        P, strat, beta_heavy,\n",
    "        n_iters_lse=15000, n_iters_polyak=300000,\n",
    "        n_restarts=80, warm_x=warm_500\n",
    "    )\n",
    "    ev = exact_val(x, P)\n",
    "    r3_results[strat] = {'val': ev, 'x': x}\n",
    "    update_global_best(ev, x, P, strat, 'round3_P500')\n",
    "\n",
    "dt = time.time() - t0\n",
    "print(f\"\\nRound 3 completed in {dt:.0f}s ({dt/60:.1f} min)\")\n",
    "print(f\"Global best so far: {global_best['val']:.6f} (P={global_best['P']}, {global_best['strategy']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e3389",
   "metadata": {},
   "source": [
    "## Round 4: Multi-Scale Warm-Start Cascade (200 -> 300 -> 500 -> 750)\n",
    "\n",
    "Take the best solutions from earlier rounds, upsample them to higher P values,\n",
    "and polish heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52905450",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ROUND 4: Multi-scale warm-start cascade (200->300->500->750)\")\n",
    "print(f\"{'='*70}\")\n",
    "t0 = time.time()\n",
    "\n",
    "# Collect best solutions at each P from previous rounds\n",
    "best_per_P = {}\n",
    "for strat, r in r1_results.items():\n",
    "    P_s = 200\n",
    "    if P_s not in best_per_P or r['val'] < best_per_P[P_s][0]:\n",
    "        best_per_P[P_s] = (r['val'], r['x'])\n",
    "\n",
    "for strat, r in r2_results.items():\n",
    "    P_s = 300\n",
    "    if P_s not in best_per_P or r['val'] < best_per_P[P_s][0]:\n",
    "        best_per_P[P_s] = (r['val'], r['x'])\n",
    "\n",
    "for strat, r in r3_results.items():\n",
    "    P_s = 500\n",
    "    if P_s not in best_per_P or r['val'] < best_per_P[P_s][0]:\n",
    "        best_per_P[P_s] = (r['val'], r['x'])\n",
    "\n",
    "# Cascade: at each level, take the best from below, upsample, then run\n",
    "# warm_perturb + dirichlet_uniform on it\n",
    "cascade_Ps = [300, 500, 750]\n",
    "\n",
    "for P_target in cascade_Ps:\n",
    "    # Find best solution at the nearest lower P\n",
    "    source_Ps = sorted([p for p in best_per_P if p < P_target], reverse=True)\n",
    "    if not source_Ps:\n",
    "        continue\n",
    "\n",
    "    P_source = source_Ps[0]\n",
    "    _, x_source = best_per_P[P_source]\n",
    "    warm_x = upsample_solution(x_source, P_source, P_target)\n",
    "\n",
    "    print(f\"\\n  Cascade: P={P_source} -> P={P_target}\")\n",
    "\n",
    "    # Heavy warm_perturb run\n",
    "    val, x, _ = hybrid_strategy_run(\n",
    "        P_target, 'warm_perturb', beta_heavy,\n",
    "        n_iters_lse=20000, n_iters_polyak=500000,\n",
    "        n_restarts=120, warm_x=warm_x\n",
    "    )\n",
    "    ev = exact_val(x, P_target)\n",
    "    update_global_best(ev, x, P_target, 'warm_perturb', f'round4_cascade_P{P_target}')\n",
    "    if P_target not in best_per_P or ev < best_per_P[P_target][0]:\n",
    "        best_per_P[P_target] = (ev, x)\n",
    "\n",
    "    # Also some fresh random starts\n",
    "    val, x, _ = hybrid_strategy_run(\n",
    "        P_target, 'dirichlet_uniform', beta_heavy,\n",
    "        n_iters_lse=20000, n_iters_polyak=500000,\n",
    "        n_restarts=60\n",
    "    )\n",
    "    ev = exact_val(x, P_target)\n",
    "    update_global_best(ev, x, P_target, 'dirichlet_uniform', f'round4_cascade_P{P_target}')\n",
    "    if ev < best_per_P[P_target][0]:\n",
    "        best_per_P[P_target] = (ev, x)\n",
    "\n",
    "dt = time.time() - t0\n",
    "print(f\"\\nRound 4 completed in {dt:.0f}s ({dt/60:.1f} min)\")\n",
    "print(f\"Global best so far: {global_best['val']:.6f} (P={global_best['P']}, {global_best['strategy']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd80aeb0",
   "metadata": {},
   "source": [
    "## Round 5: Maximum Compute at P=500 and P=750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b1886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ROUND 5: Maximum compute at P=500 and P=750\")\n",
    "print(f\"{'='*70}\")\n",
    "t0 = time.time()\n",
    "\n",
    "# Even longer beta schedule\n",
    "beta_ultra = [1, 1.3, 1.7, 2.2, 3, 4, 5.5, 7.5, 10, 14, 20, 28, 40, 55,\n",
    "              75, 100, 140, 200, 280, 400, 560, 800, 1100, 1500, 2000, 3000, 4000]\n",
    "\n",
    "for P in [500, 750]:\n",
    "    print(f\"\\n  --- P={P} ---\")\n",
    "\n",
    "    # Warm start from best available\n",
    "    if P in best_per_P:\n",
    "        warm_x = best_per_P[P][1]\n",
    "    elif global_best['x'] is not None:\n",
    "        warm_x = upsample_solution(global_best['x'], global_best['P'], P)\n",
    "    else:\n",
    "        warm_x = None\n",
    "\n",
    "    # Heavy warm_perturb: 150 restarts, max iterations\n",
    "    val, x, _ = hybrid_strategy_run(\n",
    "        P, 'warm_perturb', beta_ultra,\n",
    "        n_iters_lse=20000, n_iters_polyak=500000,\n",
    "        n_restarts=150, warm_x=warm_x\n",
    "    )\n",
    "    ev = exact_val(x, P)\n",
    "    update_global_best(ev, x, P, 'warm_perturb', f'round5_P{P}')\n",
    "    if P not in best_per_P or ev < best_per_P[P][0]:\n",
    "        best_per_P[P] = (ev, x)\n",
    "\n",
    "    # Heavy dirichlet_uniform: 100 restarts\n",
    "    val, x, _ = hybrid_strategy_run(\n",
    "        P, 'dirichlet_uniform', beta_ultra,\n",
    "        n_iters_lse=20000, n_iters_polyak=500000,\n",
    "        n_restarts=100\n",
    "    )\n",
    "    ev = exact_val(x, P)\n",
    "    update_global_best(ev, x, P, 'dirichlet_uniform', f'round5_P{P}')\n",
    "    if ev < best_per_P[P][0]:\n",
    "        best_per_P[P] = (ev, x)\n",
    "\n",
    "    # Heavy gaussian_peak: 80 restarts\n",
    "    val, x, _ = hybrid_strategy_run(\n",
    "        P, 'gaussian_peak', beta_ultra,\n",
    "        n_iters_lse=20000, n_iters_polyak=500000,\n",
    "        n_restarts=80\n",
    "    )\n",
    "    ev = exact_val(x, P)\n",
    "    update_global_best(ev, x, P, 'gaussian_peak', f'round5_P{P}')\n",
    "    if ev < best_per_P[P][0]:\n",
    "        best_per_P[P] = (ev, x)\n",
    "\n",
    "    # Heavy symmetric: 80 restarts\n",
    "    val, x, _ = hybrid_strategy_run(\n",
    "        P, 'symmetric_dirichlet', beta_ultra,\n",
    "        n_iters_lse=20000, n_iters_polyak=500000,\n",
    "        n_restarts=80\n",
    "    )\n",
    "    ev = exact_val(x, P)\n",
    "    update_global_best(ev, x, P, 'symmetric_dirichlet', f'round5_P{P}')\n",
    "    if ev < best_per_P[P][0]:\n",
    "        best_per_P[P] = (ev, x)\n",
    "\n",
    "dt = time.time() - t0\n",
    "print(f\"\\nRound 5 completed in {dt:.0f}s ({dt/60:.1f} min)\")\n",
    "print(f\"Global best so far: {global_best['val']:.6f} (P={global_best['P']}, {global_best['strategy']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edcd0c7",
   "metadata": {},
   "source": [
    "## Round 6: Final Push at P=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dddc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 1000\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ROUND 6: Final push at P={P}\")\n",
    "print(f\"{'='*70}\")\n",
    "t0 = time.time()\n",
    "\n",
    "# Upsample best from highest available P\n",
    "warm_1000 = upsample_solution(best_per_P[max(best_per_P.keys())][1],\n",
    "                               max(best_per_P.keys()), P)\n",
    "\n",
    "# Heavy warm_perturb\n",
    "val, x, _ = hybrid_strategy_run(\n",
    "    P, 'warm_perturb', beta_ultra,\n",
    "    n_iters_lse=15000, n_iters_polyak=500000,\n",
    "    n_restarts=100, warm_x=warm_1000\n",
    ")\n",
    "ev = exact_val(x, P)\n",
    "update_global_best(ev, x, P, 'warm_perturb', 'round6_P1000')\n",
    "if P not in best_per_P or ev < best_per_P[P][0]:\n",
    "    best_per_P[P] = (ev, x)\n",
    "\n",
    "# Fresh random starts\n",
    "val, x, _ = hybrid_strategy_run(\n",
    "    P, 'dirichlet_uniform', beta_ultra,\n",
    "    n_iters_lse=15000, n_iters_polyak=500000,\n",
    "    n_restarts=60\n",
    ")\n",
    "ev = exact_val(x, P)\n",
    "update_global_best(ev, x, P, 'dirichlet_uniform', 'round6_P1000')\n",
    "if ev < best_per_P[P][0]:\n",
    "    best_per_P[P] = (ev, x)\n",
    "\n",
    "# Gaussian peaks\n",
    "val, x, _ = hybrid_strategy_run(\n",
    "    P, 'gaussian_peak', beta_ultra,\n",
    "    n_iters_lse=15000, n_iters_polyak=500000,\n",
    "    n_restarts=60\n",
    ")\n",
    "ev = exact_val(x, P)\n",
    "update_global_best(ev, x, P, 'gaussian_peak', 'round6_P1000')\n",
    "if ev < best_per_P[P][0]:\n",
    "    best_per_P[P] = (ev, x)\n",
    "\n",
    "dt = time.time() - t0\n",
    "print(f\"\\nRound 6 completed in {dt:.0f}s ({dt/60:.1f} min)\")\n",
    "print(f\"Global best so far: {global_best['val']:.6f} (P={global_best['P']}, {global_best['strategy']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aedf2ce",
   "metadata": {},
   "source": [
    "## Round 7: Cross-Pollination\n",
    "\n",
    "Blend the best solutions from different P values and strategies via random convex\n",
    "combinations, then re-optimize. This explores the space between known good solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd6b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ROUND 7: Cross-pollination (blend best solutions + re-optimize)\")\n",
    "print(f\"{'='*70}\")\n",
    "t0 = time.time()\n",
    "\n",
    "# For each target P, create blended initializations from multiple source P levels\n",
    "for P_target in [500, 750, 1000]:\n",
    "    print(f\"\\n  --- Cross-pollination at P={P_target} ---\")\n",
    "\n",
    "    # Collect all available solutions, upsample to P_target\n",
    "    upsampled = []\n",
    "    for P_src, (v, x_src) in best_per_P.items():\n",
    "        if P_src != P_target:\n",
    "            x_up = upsample_solution(x_src, P_src, P_target)\n",
    "        else:\n",
    "            x_up = x_src.copy()\n",
    "        upsampled.append(x_up)\n",
    "\n",
    "    if len(upsampled) < 2:\n",
    "        continue\n",
    "\n",
    "    # Generate blended inits: random convex combinations of 2-3 solutions\n",
    "    rng = np.random.default_rng()\n",
    "    n_blend = 100\n",
    "    blend_inits = []\n",
    "    for _ in range(n_blend):\n",
    "        k = rng.integers(2, min(4, len(upsampled) + 1))\n",
    "        idxs = rng.choice(len(upsampled), size=k, replace=False)\n",
    "        weights = rng.dirichlet(np.ones(k))\n",
    "        x_blend = sum(w * upsampled[i] for i, w in zip(idxs, weights))\n",
    "        # Add small noise\n",
    "        x_blend += 0.3 * rng.standard_normal(P_target) * np.mean(x_blend)\n",
    "        x_blend = np.maximum(x_blend, 0.0)\n",
    "        x_blend /= x_blend.sum()\n",
    "        blend_inits.append(x_blend)\n",
    "\n",
    "    # Run hybrid on blended inits\n",
    "    beta_arr = np.array(beta_ultra, dtype=np.float64)\n",
    "    results_blend = Parallel(n_jobs=-1, verbose=0)(\n",
    "        delayed(_hybrid_single_restart)(blend_inits[i], P_target, beta_arr, 15000, 500000)\n",
    "        for i in range(n_blend)\n",
    "    )\n",
    "\n",
    "    best_val = np.inf\n",
    "    best_x = None\n",
    "    for lse_v, pol_v, x in results_blend:\n",
    "        if pol_v < best_val:\n",
    "            best_val = pol_v\n",
    "            best_x = x.copy()\n",
    "\n",
    "    ev = exact_val(best_x, P_target)\n",
    "    print(f\"    Blended best: {ev:.6f}\")\n",
    "    update_global_best(ev, best_x, P_target, 'cross_pollination', f'round7_P{P_target}')\n",
    "    if P_target not in best_per_P or ev < best_per_P[P_target][0]:\n",
    "        best_per_P[P_target] = (ev, best_x)\n",
    "\n",
    "dt = time.time() - t0\n",
    "print(f\"\\nRound 7 completed in {dt:.0f}s ({dt/60:.1f} min)\")\n",
    "print(f\"Global best so far: {global_best['val']:.6f} (P={global_best['P']}, {global_best['strategy']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1745ef9e",
   "metadata": {},
   "source": [
    "## Final Summary and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3ad178",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = time.time() - t_total_start\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"HEAVY COMPUTE COMPLETE -- Total time: {t_total:.0f}s ({t_total/60:.1f} min, {t_total/3600:.2f} hr)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'P':>6} | {'Best value':>14} | {'Source':>20}\")\n",
    "print('-' * 50)\n",
    "for P_s in sorted(best_per_P.keys()):\n",
    "    v, _ = best_per_P[P_s]\n",
    "    print(f\"{P_s:>6} | {v:>14.6f} |\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"GLOBAL BEST: {global_best['val']:.6f}\")\n",
    "print(f\"  P = {global_best['P']}\")\n",
    "print(f\"  Strategy = {global_best['strategy']}\")\n",
    "print(f\"  Round = {global_best['round']}\")\n",
    "print(f\"  Best known in literature: 1.5029\")\n",
    "print(f\"  Gap to literature: {global_best['val'] - 1.5029:+.6f}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save all best solutions\n",
    "save_data = {}\n",
    "for P_s in sorted(best_per_P.keys()):\n",
    "    v, x = best_per_P[P_s]\n",
    "    edges = np.linspace(-0.25, 0.25, P_s + 1)\n",
    "    bin_width = 0.5 / P_s\n",
    "    save_data[f\"heavy_P{P_s}\"] = {\n",
    "        'P': P_s,\n",
    "        'exact_peak': v,\n",
    "        'simplex_weights': x.tolist(),\n",
    "        'edges': edges.tolist(),\n",
    "        'heights': (x / bin_width).tolist(),\n",
    "    }\n",
    "\n",
    "# Also save global best separately\n",
    "save_data['global_best'] = {\n",
    "    'P': global_best['P'],\n",
    "    'exact_peak': global_best['val'],\n",
    "    'strategy': global_best['strategy'],\n",
    "    'round': global_best['round'],\n",
    "    'simplex_weights': global_best['x'].tolist(),\n",
    "}\n",
    "\n",
    "out_path = os.path.join('.', 'best_solutions.json')\n",
    "with open(out_path, 'w') as f:\n",
    "    json.dump(save_data, f, indent=2)\n",
    "print(f\"\\nSaved {len(save_data)} solutions to {out_path}\")\n",
    "\n",
    "# Visualization of global best\n",
    "P_best = global_best['P']\n",
    "x_best = global_best['x']\n",
    "edges = np.linspace(-0.25, 0.25, P_best + 1)\n",
    "centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "bin_width = 0.5 / P_best\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.bar(centers, x_best / bin_width, width=bin_width * 0.9,\n",
    "        color='C0', edgecolor='k', linewidth=0.2, alpha=0.8)\n",
    "ax1.set_title(f'Global best: P={P_best}, peak={global_best[\"val\"]:.6f}\\n'\n",
    "              f'Strategy: {global_best[\"strategy\"]}, Round: {global_best[\"round\"]}')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('f(x)')\n",
    "\n",
    "# Convergence across P values\n",
    "Ps_sorted = sorted(best_per_P.keys())\n",
    "vals_sorted = [best_per_P[p][0] for p in Ps_sorted]\n",
    "ax2.plot(Ps_sorted, vals_sorted, 'o-', markersize=8, linewidth=2)\n",
    "ax2.axhline(1.5029, color='r', linestyle='--', alpha=0.5, label='Literature best (1.5029)')\n",
    "ax2.set_xlabel('P (number of bins)')\n",
    "ax2.set_ylabel('Peak autoconvolution (exact)')\n",
    "ax2.set_title('Best value vs discretization')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('heavy_compute_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
