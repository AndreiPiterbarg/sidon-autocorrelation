{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Adaptive Grid Optimization for $C_{1a}$\n",
    "\n",
    "## Summary\n",
    "- **Method**: Non-uniform grid refinement + L-BFGS-B polish (with optional DE)\n",
    "- **Key idea**: All prior work uses uniform grids. The extremizer likely has structure (boundary effects, cusps) that benefits from non-uniform discretization.\n",
    "- **Best result**: 1.537413 (P=500, exact), starting from uniform P=200 solution (1.509766)\n",
    "- **Outcome**: Adaptive grids did NOT improve over uniform grid baseline. Non-uniform grids introduce interpolation artifacts that L-BFGS cannot overcome. The FFT evaluation is also less accurate on non-uniform grids.\n",
    "\n",
    "## Approach\n",
    "1. Start from the best known uniform-grid solution (P=200, peak=1.5098)\n",
    "2. Identify regions of high curvature/structure in f\n",
    "3. Refine grid in those regions (curvature + boundary weighting)\n",
    "4. Re-optimize on the non-uniform grid via L-BFGS-B\n",
    "5. Iterate: refine -> optimize -> refine -> optimize\n",
    "\n",
    "## Key findings\n",
    "- DE (differential evolution) converges to ~1.7-1.8 on non-uniform grids â€” far from optimal\n",
    "- L-BFGS-B from interpolated warm-start stays near ~1.54, worse than starting uniform solution\n",
    "- Width ratios up to 114:1 cause FFT evaluation errors of ~0.005\n",
    "- **Conclusion**: Adaptive grids are not useful without a fundamentally different optimizer that handles non-uniform discretization natively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from numba import njit, prange\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(f\"NumPy {np.__version__}\")\n",
    "print(f\"CPU cores available: {np.os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numba-kernels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Numba JIT-compiled exact autoconvolution ===\n",
    "\n",
    "@njit(cache=True)\n",
    "def _autoconv_at_points(edges, heights, t_values):\n",
    "    \"\"\"Compute autoconvolution at given t values.\"\"\"\n",
    "    N = len(heights)\n",
    "    T = len(t_values)\n",
    "    result = np.zeros(T)\n",
    "    a = edges[:-1]\n",
    "    b = edges[1:]\n",
    "    for ti in range(T):\n",
    "        t = t_values[ti]\n",
    "        total = 0.0\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                lo = max(a[i], t - b[j])\n",
    "                hi = min(b[i], t - a[j])\n",
    "                if hi > lo:\n",
    "                    total += heights[i] * heights[j] * (hi - lo)\n",
    "        result[ti] = total\n",
    "    return result\n",
    "\n",
    "\n",
    "@njit(parallel=True, cache=True)\n",
    "def _autoconv_at_points_parallel(edges, heights, t_values):\n",
    "    \"\"\"Parallel version for large number of t values.\"\"\"\n",
    "    N = len(heights)\n",
    "    T = len(t_values)\n",
    "    result = np.zeros(T)\n",
    "    a = edges[:-1]\n",
    "    b = edges[1:]\n",
    "    for ti in prange(T):\n",
    "        t = t_values[ti]\n",
    "        total = 0.0\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                lo = max(a[i], t - b[j])\n",
    "                hi = min(b[i], t - a[j])\n",
    "                if hi > lo:\n",
    "                    total += heights[i] * heights[j] * (hi - lo)\n",
    "        result[ti] = total\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_breakpoints(edges):\n",
    "    \"\"\"All pairwise edge sums where autoconvolution can peak.\"\"\"\n",
    "    bp = (edges[:, None] + edges[None, :]).ravel()\n",
    "    bp = np.unique(bp)\n",
    "    return bp[(bp >= -0.5) & (bp <= 0.5)]\n",
    "\n",
    "\n",
    "def peak_autoconv_exact(heights, edges):\n",
    "    \"\"\"Compute exact peak using Numba-accelerated evaluation.\"\"\"\n",
    "    bp = compute_breakpoints(edges)\n",
    "    if len(bp) > 500:\n",
    "        conv = _autoconv_at_points_parallel(edges, heights, bp)\n",
    "    else:\n",
    "        conv = _autoconv_at_points(edges, heights, bp)\n",
    "    return float(np.max(conv))\n",
    "\n",
    "\n",
    "def peak_autoconv_fft(heights, edges, n_grid=8192):\n",
    "    \"\"\"Fast FFT-based peak estimation. NOT exact but O(N log N).\"\"\"\n",
    "    dx = 0.5 / n_grid\n",
    "    x = -0.25 + np.arange(n_grid) * dx\n",
    "    idx = np.searchsorted(edges, x, side='right') - 1\n",
    "    idx = np.clip(idx, 0, len(heights) - 1)\n",
    "    f_vals = heights[idx]\n",
    "    f_vals = np.where((x < edges[0]) | (x >= edges[-1]), 0.0, f_vals)\n",
    "    f_padded = np.zeros(2 * n_grid)\n",
    "    f_padded[:n_grid] = f_vals\n",
    "    F = np.fft.rfft(f_padded)\n",
    "    conv_padded = np.fft.irfft(F * F) * dx\n",
    "    t_values = -0.5 + np.arange(2 * n_grid) * dx\n",
    "    mask = (t_values >= -0.5) & (t_values <= 0.5)\n",
    "    return float(np.max(conv_padded[mask]))\n",
    "\n",
    "\n",
    "# Warm up JIT\n",
    "print(\"Warming up Numba JIT...\")\n",
    "_e = np.linspace(-0.25, 0.25, 51)\n",
    "_h = np.ones(50) * 2.0\n",
    "_bp = compute_breakpoints(_e)\n",
    "_ = _autoconv_at_points(_e, _h, _bp[:10])\n",
    "_ = _autoconv_at_points_parallel(_e, _h, _bp)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "core-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Core functions ===\n",
    "\n",
    "def normalize_heights(heights, widths):\n",
    "    h = np.maximum(heights, 0.0)\n",
    "    total = np.sum(h * widths)\n",
    "    if total > 1e-15:\n",
    "        h = h / total\n",
    "    return h\n",
    "\n",
    "\n",
    "def objective_fft(x, edges, widths):\n",
    "    heights = normalize_heights(x, widths)\n",
    "    return peak_autoconv_fft(heights, edges)\n",
    "\n",
    "\n",
    "def optimize_lbfgs(heights_init, edges, maxiter=500, verbose=True):\n",
    "    widths = np.diff(edges)\n",
    "    n = len(heights_init)\n",
    "    bounds = [(0, None)] * n\n",
    "    iter_count = [0]\n",
    "    best_val = [np.inf]\n",
    "    def callback(xk):\n",
    "        iter_count[0] += 1\n",
    "        val = objective_fft(xk, edges, widths)\n",
    "        if val < best_val[0]:\n",
    "            best_val[0] = val\n",
    "        if verbose and iter_count[0] % 50 == 0:\n",
    "            print(f\"      L-BFGS iter {iter_count[0]}: best={best_val[0]:.6f}\")\n",
    "    result = minimize(objective_fft, heights_init.copy(), args=(edges, widths),\n",
    "                      method='L-BFGS-B', bounds=bounds, callback=callback,\n",
    "                      options={'maxiter': maxiter, 'disp': False})\n",
    "    best_heights = normalize_heights(result.x, widths)\n",
    "    return peak_autoconv_fft(best_heights, edges), best_heights\n",
    "\n",
    "\n",
    "def compute_curvature(heights, edges):\n",
    "    n = len(heights)\n",
    "    scores = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        if i > 0: scores[i] += abs(heights[i] - heights[i - 1])\n",
    "        if i < n - 1: scores[i] += abs(heights[i] - heights[i + 1])\n",
    "    h_max = max(heights.max(), 1e-10)\n",
    "    scores += 0.5 * heights / h_max\n",
    "    centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "    scores += 0.3 * np.exp(-50 * (0.25 - np.abs(centers)) ** 2)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def refine_grid(heights, edges, target_bins, blend=0.5):\n",
    "    scores = compute_curvature(heights, edges)\n",
    "    widths = np.diff(edges)\n",
    "    density = scores / widths\n",
    "    density = density / density.sum()\n",
    "    uniform = np.ones(len(heights)) / len(heights)\n",
    "    density = (1 - blend) * density + blend * uniform\n",
    "    density = density / density.sum()\n",
    "    cdf = np.concatenate([[0], np.cumsum(density * widths)])\n",
    "    cdf = cdf / cdf[-1]\n",
    "    new_edges = np.interp(np.linspace(0, 1, target_bins + 1), cdf, edges)\n",
    "    new_edges[0], new_edges[-1] = edges[0], edges[-1]\n",
    "    return new_edges\n",
    "\n",
    "\n",
    "def interpolate_heights(old_heights, old_edges, new_edges):\n",
    "    old_centers = 0.5 * (old_edges[:-1] + old_edges[1:])\n",
    "    new_centers = 0.5 * (new_edges[:-1] + new_edges[1:])\n",
    "    new_heights = np.maximum(np.interp(new_centers, old_centers, old_heights), 0)\n",
    "    new_widths = np.diff(new_edges)\n",
    "    integral = np.sum(new_heights * new_widths)\n",
    "    if integral > 1e-15:\n",
    "        new_heights = new_heights / integral\n",
    "    return new_heights\n",
    "\n",
    "\n",
    "print(\"All functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-best",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load best known solution ===\n",
    "with open('best_solutions.json', 'r') as f:\n",
    "    solutions = json.load(f)\n",
    "\n",
    "start_sol = solutions['heavy_P200']\n",
    "P_start = start_sol['P']\n",
    "start_edges = np.array(start_sol['edges'])\n",
    "start_heights = np.array(start_sol['heights'])\n",
    "start_peak = start_sol['exact_peak']\n",
    "print(f\"Starting from heavy_P200: P={P_start}, peak={start_peak:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Main adaptive grid optimization ===\n",
    "# Uses L-BFGS only (DE was found to converge to ~1.7-1.8 on non-uniform grids)\n",
    "\n",
    "target_bins_seq = [200, 250, 300, 400, 500]\n",
    "blend_seq = [0.15, 0.12, 0.1, 0.08, 0.05]\n",
    "\n",
    "heights, edges = start_heights.copy(), start_edges.copy()\n",
    "global_best_val, global_best_heights, global_best_edges = np.inf, None, None\n",
    "results = []\n",
    "t_start = time.time()\n",
    "\n",
    "for stage, (target_bins, blend) in enumerate(zip(target_bins_seq, blend_seq)):\n",
    "    print(f\"\\nSTAGE {stage+1}/{len(target_bins_seq)}: {target_bins} bins, blend={blend}\")\n",
    "    new_edges = refine_grid(heights, edges, target_bins, blend=blend)\n",
    "    new_heights = interpolate_heights(heights, edges, new_edges)\n",
    "    widths = np.diff(new_edges)\n",
    "    print(f\"  Grid: min_w={widths.min():.5f}, max_w={widths.max():.5f}, ratio={widths.max()/widths.min():.1f}\")\n",
    "    \n",
    "    best_val_fft = peak_autoconv_fft(new_heights, new_edges)\n",
    "    best_h = new_heights.copy()\n",
    "    \n",
    "    for r in range(5):  # 5 L-BFGS restarts\n",
    "        if r == 0: h_init = new_heights.copy()\n",
    "        elif r < 3: h_init = np.maximum(best_h + np.random.randn(target_bins) * 0.05*(r+1) * best_h.mean(), 0)\n",
    "        else: h_init = np.maximum(best_h * (1 + 0.1*np.random.randn(target_bins)), 0)\n",
    "        try:\n",
    "            val, h = optimize_lbfgs(h_init, new_edges, 500, verbose=False)\n",
    "            if val < best_val_fft:\n",
    "                best_val_fft, best_h = val, h.copy()\n",
    "                print(f\"    restart {r+1}: {val:.6f} <- improved\")\n",
    "        except: pass\n",
    "    \n",
    "    exact_val = peak_autoconv_exact(best_h, new_edges)\n",
    "    print(f\"  FFT={best_val_fft:.6f}, EXACT={exact_val:.6f}\")\n",
    "    results.append({'bins': target_bins, 'exact_peak': exact_val})\n",
    "    \n",
    "    if exact_val < global_best_val:\n",
    "        global_best_val = exact_val\n",
    "        global_best_heights, global_best_edges = best_h.copy(), new_edges.copy()\n",
    "        print(f\"  *** NEW BEST: {exact_val:.6f} ***\")\n",
    "    heights, edges = best_h, new_edges\n",
    "\n",
    "print(f\"\\nDone in {time.time()-t_start:.1f}s. Best: {global_best_val:.6f}\")\n",
    "print(f\"Gap to literature: {global_best_val - 1.5029:+.6f}\")\n",
    "print(f\"Improvement from start: {start_peak - global_best_val:+.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "| Bins | Exact Peak |\n",
    "|------|------------|\n",
    "| 200  | 1.541514   |\n",
    "| 250  | 1.542924   |\n",
    "| 300  | 1.540497   |\n",
    "| 400  | 1.539041   |\n",
    "| 500  | 1.537413   |\n",
    "\n",
    "**Conclusion**: The adaptive grid approach **worsened** the objective from 1.5098 (uniform P=200 baseline) to 1.5374 (best adaptive). The non-uniform grid introduces:\n",
    "1. Interpolation artifacts when transferring solutions between grids\n",
    "2. FFT evaluation errors from extreme width ratios (up to 114:1)\n",
    "3. L-BFGS gets trapped in worse local minima on the distorted landscape\n",
    "\n",
    "This partially answers K2 (boundary singularity question): if a singularity exists, it cannot be exploited by naive grid refinement + standard optimizers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}