{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Joint Edge + Height Optimization for Autoconvolution Minimization\n",
    "\n",
    "Optimizes **both bin edges and heights** simultaneously as one parameter vector\n",
    "`theta = (gamma, eta)`, avoiding any grid interpolation.\n",
    "\n",
    "### Method\n",
    "- **Parametrization**: `gamma` controls bin widths via softplus + normalization;\n",
    "  `eta` controls bin heights via softplus + normalization. All constraints\n",
    "  (positive widths summing to 0.5, positive heights with integral 1) are\n",
    "  satisfied by construction.\n",
    "- **Objective**: LogSumExp smooth approximation to `max_t (f*f)(t)`, with\n",
    "  smooth log-barrier penalty on width ratio to prevent degenerate bins.\n",
    "- **Speed**: Autoconvolution computed via **analytical Fourier transform** of\n",
    "  the step function — O(NP) per evaluation vs O(P^4) for exact breakpoint\n",
    "  evaluation. Exact breakpoint evaluation used only for final verification.\n",
    "- **Gradients**: Analytical backprop through LSE → irfft → F² → Fourier coeffs →\n",
    "  reparametrization. O(NP + N log N) per gradient vs O(NP²) for finite differences.\n",
    "- **Optimizer**: L-BFGS-B with analytical gradients, beta-continuation\n",
    "  from beta=1 to beta=2000+, multi-start with 30-50 random restarts.\n",
    "- **Width ratio penalty**: smooth p-norm ratio approximating max(w)/min(w),\n",
    "  with log-barrier `λ·log(1 + (ratio/R)²)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numba 0.63.1, NumPy 2.3.5\n",
      "CPU cores: 16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numba as nb\n",
    "from numba import njit\n",
    "from scipy.optimize import minimize\n",
    "from joblib import Parallel, delayed\n",
    "import json, os, time\n",
    "\n",
    "print(f'Numba {nb.__version__}, NumPy {np.__version__}')\n",
    "print(f'CPU cores: {os.cpu_count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Core Numba Kernels\n",
    "\n",
    "Softplus reparametrization, exact autoconvolution at breakpoints (for verification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform P=5: exact peak = 2.000000 (expected ~2.0 for constant f=2)\n",
      "Numba kernels compiled.\n"
     ]
    }
   ],
   "source": [
    "@njit(cache=True)\n",
    "def softplus(x):\n",
    "    if x > 20.0:\n",
    "        return x\n",
    "    elif x < -20.0:\n",
    "        return np.exp(x)\n",
    "    else:\n",
    "        return np.log1p(np.exp(x))\n",
    "\n",
    "\n",
    "@njit(cache=True)\n",
    "def softplus_vec(x):\n",
    "    out = np.empty(len(x))\n",
    "    for i in range(len(x)):\n",
    "        out[i] = softplus(x[i])\n",
    "    return out\n",
    "\n",
    "\n",
    "@njit(cache=True)\n",
    "def sigmoid(x):\n",
    "    \"\"\"Numerically stable sigmoid = softplus'.\"\"\"\n",
    "    if x >= 0.0:\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    else:\n",
    "        ex = np.exp(x)\n",
    "        return ex / (1.0 + ex)\n",
    "\n",
    "\n",
    "@njit(cache=True)\n",
    "def theta_to_edges_heights(gamma, eta):\n",
    "    \"\"\"Convert unconstrained (gamma, eta) -> (edges, heights, widths).\n",
    "    gamma -> widths via softplus + normalize to sum=0.5\n",
    "    eta   -> heights via softplus + normalize so sum(h*w)=1\n",
    "    \"\"\"\n",
    "    P = len(gamma)\n",
    "    raw_w = softplus_vec(gamma)\n",
    "    w_sum = 0.0\n",
    "    for i in range(P):\n",
    "        w_sum += raw_w[i]\n",
    "    widths = np.empty(P)\n",
    "    for i in range(P):\n",
    "        widths[i] = 0.5 * raw_w[i] / w_sum\n",
    "\n",
    "    edges = np.empty(P + 1)\n",
    "    edges[0] = -0.25\n",
    "    for i in range(P):\n",
    "        edges[i + 1] = edges[i] + widths[i]\n",
    "    edges[P] = 0.25  # force exact endpoint\n",
    "\n",
    "    raw_h = softplus_vec(eta)\n",
    "    hw_sum = 0.0\n",
    "    for i in range(P):\n",
    "        hw_sum += raw_h[i] * widths[i]\n",
    "    heights = np.empty(P)\n",
    "    for i in range(P):\n",
    "        heights[i] = raw_h[i] / hw_sum\n",
    "\n",
    "    return edges, heights, widths\n",
    "\n",
    "\n",
    "@njit(cache=True)\n",
    "def peak_exact(edges, heights):\n",
    "    \"\"\"Exact peak of (f*f)(t) by evaluating at all O(P^2) breakpoints.\"\"\"\n",
    "    P = len(heights)\n",
    "    P1 = P + 1\n",
    "    a = edges[:-1]\n",
    "    b = edges[1:]\n",
    "\n",
    "    # Collect unique breakpoints\n",
    "    raw = np.empty(P1 * P1)\n",
    "    k = 0\n",
    "    for i in range(P1):\n",
    "        for j in range(P1):\n",
    "            raw[k] = edges[i] + edges[j]\n",
    "            k += 1\n",
    "    raw = np.sort(raw)\n",
    "    bp = np.empty(len(raw))\n",
    "    bp[0] = raw[0]\n",
    "    n_bp = 1\n",
    "    for i in range(1, len(raw)):\n",
    "        if raw[i] - bp[n_bp - 1] > 1e-15:\n",
    "            bp[n_bp] = raw[i]\n",
    "            n_bp += 1\n",
    "\n",
    "    mx = -1e300\n",
    "    for ti in range(n_bp):\n",
    "        t = bp[ti]\n",
    "        total = 0.0\n",
    "        for i in range(P):\n",
    "            for j in range(P):\n",
    "                lo = a[i] if a[i] > t - b[j] else t - b[j]\n",
    "                hi = b[i] if b[i] < t - a[j] else t - a[j]\n",
    "                if hi > lo:\n",
    "                    total += heights[i] * heights[j] * (hi - lo)\n",
    "        if total > mx:\n",
    "            mx = total\n",
    "    return mx\n",
    "\n",
    "\n",
    "# JIT warmup\n",
    "_g = np.zeros(5)\n",
    "_e, _h, _w = theta_to_edges_heights(_g, _g)\n",
    "_p = peak_exact(_e, _h)\n",
    "_ = sigmoid(0.0)\n",
    "print(f'Uniform P=5: exact peak = {_p:.6f} (expected ~2.0 for constant f=2)')\n",
    "print('Numba kernels compiled.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## FFT-Based Autoconvolution via Analytical Fourier Transform\n",
    "\n",
    "For a step function $f(x) = h_i$ on $[e_i, e_{i+1})$, the Fourier transform is:\n",
    "$$\\hat{F}(\\omega) = \\sum_i h_i \\frac{e^{-j\\omega e_i} - e^{-j\\omega e_{i+1}}}{j\\omega}$$\n",
    "\n",
    "The autoconvolution Fourier series coefficients are $\\hat{g}_k = \\hat{F}(2\\pi k)^2$.\n",
    "Inverse FFT recovers $(f*f)(t)$ on a uniform grid. This is $O(NP)$ per evaluation\n",
    "instead of $O(P^4)$ for exact breakpoint evaluation.\n",
    "\n",
    "**Analytical gradients**: Backprop through the chain\n",
    "LSE → fftshift → irfft → complex squaring → Fourier coefficients → reparametrization,\n",
    "giving exact gradients in $O(NP + N\\log N)$ vs $O(NP^2)$ for finite differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform f=2, P=10:\n",
      "  FFT peak:   1.999901\n",
      "  Exact peak: 2.000000\n",
      "  Error:      9.89e-05\n",
      "Non-uniform P=5:\n",
      "  FFT peak:   2.160459\n",
      "  Exact peak: 2.160494\n",
      "  Error:      3.49e-05\n",
      "Backward kernels compiled.\n"
     ]
    }
   ],
   "source": [
    "@njit(cache=True)\n",
    "def fourier_coeffs(edges, heights, n_freq):\n",
    "    \"\"\"Compute Fourier transform F_hat(2*pi*k) for k=0..n_freq-1.\n",
    "    Returns real and imaginary parts as separate arrays.\n",
    "    \"\"\"\n",
    "    P = len(heights)\n",
    "    re = np.zeros(n_freq)\n",
    "    im = np.zeros(n_freq)\n",
    "    re[0] = 1.0  # DC = integral f = 1\n",
    "\n",
    "    for k in range(1, n_freq):\n",
    "        omega = 2.0 * np.pi * k\n",
    "        sr = 0.0\n",
    "        si = 0.0\n",
    "        for i in range(P):\n",
    "            c0 = np.cos(omega * edges[i])\n",
    "            s0 = np.sin(omega * edges[i])\n",
    "            c1 = np.cos(omega * edges[i + 1])\n",
    "            s1 = np.sin(omega * edges[i + 1])\n",
    "            a = c0 - c1\n",
    "            b = s1 - s0\n",
    "            sr += heights[i] * b / omega\n",
    "            si += heights[i] * (-a) / omega\n",
    "        re[k] = sr\n",
    "        im[k] = si\n",
    "    return re, im\n",
    "\n",
    "\n",
    "@njit(cache=True)\n",
    "def fourier_backward(edges, heights, d_F_re, d_F_im, n_freq):\n",
    "    \"\"\"Backward pass: gradients of loss w.r.t. edges and heights,\n",
    "    given gradients w.r.t. Fourier coefficients (d_F_re, d_F_im).\n",
    "    \"\"\"\n",
    "    P = len(heights)\n",
    "    d_edges = np.zeros(P + 1)\n",
    "    d_heights = np.zeros(P)\n",
    "\n",
    "    for k in range(1, n_freq):\n",
    "        omega = 2.0 * np.pi * k\n",
    "        for i in range(P):\n",
    "            cos_lo = np.cos(omega * edges[i])\n",
    "            sin_lo = np.sin(omega * edges[i])\n",
    "            cos_hi = np.cos(omega * edges[i + 1])\n",
    "            sin_hi = np.sin(omega * edges[i + 1])\n",
    "\n",
    "            # dL/dh_i: F_re += h_i*(sin_hi-sin_lo)/w, F_im += h_i*(cos_hi-cos_lo)/w\n",
    "            d_heights[i] += (d_F_re[k] * (sin_hi - sin_lo)\n",
    "                             + d_F_im[k] * (cos_hi - cos_lo)) / omega\n",
    "\n",
    "            # dL/de_{i+1}: d/de sin(we)/w = cos(we), d/de cos(we)/w = -sin(we)\n",
    "            d_edges[i + 1] += heights[i] * (\n",
    "                d_F_re[k] * cos_hi - d_F_im[k] * sin_hi)\n",
    "\n",
    "            # dL/de_i: sign flips (these terms enter with minus)\n",
    "            d_edges[i] += heights[i] * (\n",
    "                -d_F_re[k] * cos_lo + d_F_im[k] * sin_lo)\n",
    "\n",
    "    return d_edges, d_heights\n",
    "\n",
    "\n",
    "@njit(cache=True)\n",
    "def reparam_backward(gamma, eta, widths, heights, d_edges, d_heights,\n",
    "                     d_penalty_widths):\n",
    "    \"\"\"Backward pass through theta_to_edges_heights reparametrization.\n",
    "    Combines gradients from edges, heights, and the penalty term on widths\n",
    "    to produce gradients w.r.t. the unconstrained parameters (gamma, eta).\n",
    "    \"\"\"\n",
    "    P = len(gamma)\n",
    "\n",
    "    # Recompute intermediates needed for backward pass\n",
    "    raw_w = softplus_vec(gamma)\n",
    "    w_sum = 0.0\n",
    "    for i in range(P):\n",
    "        w_sum += raw_w[i]\n",
    "\n",
    "    raw_h = softplus_vec(eta)\n",
    "    hw_sum = 0.0\n",
    "    for i in range(P):\n",
    "        hw_sum += raw_h[i] * widths[i]\n",
    "\n",
    "    # --- d_edges -> d_widths via reverse cumsum ---\n",
    "    # edges[j] = -0.25 + sum_{i<j} widths[i], for j=1..P-1\n",
    "    # edges[0]=-0.25 and edges[P]=0.25 are constants -> discard their grads\n",
    "    d_widths = np.zeros(P)\n",
    "    acc = 0.0\n",
    "    for j in range(P - 1, 0, -1):\n",
    "        acc += d_edges[j]\n",
    "        d_widths[j - 1] = acc\n",
    "    # d_widths[P-1] stays 0 (edges[P] is forced constant)\n",
    "\n",
    "    # --- d_heights -> d_raw_h and additional d_widths ---\n",
    "    # heights_i = raw_h_i / hw_sum, hw_sum = sum(raw_h * widths)\n",
    "    S_dh = 0.0\n",
    "    for i in range(P):\n",
    "        S_dh += d_heights[i] * heights[i]\n",
    "\n",
    "    d_raw_h = np.empty(P)\n",
    "    for i in range(P):\n",
    "        d_raw_h[i] = (d_heights[i] - widths[i] * S_dh) / hw_sum\n",
    "        d_widths[i] += -heights[i] * S_dh\n",
    "\n",
    "    # --- Add penalty gradient on widths ---\n",
    "    for i in range(P):\n",
    "        d_widths[i] += d_penalty_widths[i]\n",
    "\n",
    "    # --- d_widths -> d_raw_w ---\n",
    "    # widths_i = 0.5 * raw_w_i / w_sum\n",
    "    S_dw = 0.0\n",
    "    for i in range(P):\n",
    "        S_dw += d_widths[i] * widths[i]\n",
    "\n",
    "    d_raw_w = np.empty(P)\n",
    "    for i in range(P):\n",
    "        d_raw_w[i] = (0.5 / w_sum) * (d_widths[i] - 2.0 * S_dw)\n",
    "\n",
    "    # --- Through softplus: d_gamma = d_raw_w * sigmoid(gamma), same for eta ---\n",
    "    d_gamma = np.empty(P)\n",
    "    d_eta = np.empty(P)\n",
    "    for i in range(P):\n",
    "        d_gamma[i] = d_raw_w[i] * sigmoid(gamma[i])\n",
    "        d_eta[i] = d_raw_h[i] * sigmoid(eta[i])\n",
    "\n",
    "    return d_gamma, d_eta\n",
    "\n",
    "\n",
    "@njit(cache=True)\n",
    "def square_complex(re, im):\n",
    "    \"\"\"Compute (re + j*im)^2 = (re^2 - im^2) + j*(2*re*im).\"\"\"\n",
    "    n = len(re)\n",
    "    out_re = np.empty(n)\n",
    "    out_im = np.empty(n)\n",
    "    for i in range(n):\n",
    "        out_re[i] = re[i] * re[i] - im[i] * im[i]\n",
    "        out_im[i] = 2.0 * re[i] * im[i]\n",
    "    return out_re, out_im\n",
    "\n",
    "\n",
    "def fft_autoconv(edges, heights, N_fft=16384):\n",
    "    \"\"\"Compute autoconvolution on a grid via analytical Fourier transform.\n",
    "    Returns array of (f*f)(t) values at t = -0.5 + m/N_fft, m=0..N_fft-1.\n",
    "    \"\"\"\n",
    "    n_freq = N_fft // 2 + 1\n",
    "    f_re, f_im = fourier_coeffs(edges, heights, n_freq)\n",
    "    g_re, g_im = square_complex(f_re, f_im)\n",
    "\n",
    "    G = g_re + 1j * g_im\n",
    "    conv = np.fft.irfft(G, n=N_fft) * N_fft\n",
    "    conv = np.fft.fftshift(conv)\n",
    "    return conv\n",
    "\n",
    "\n",
    "def lse_from_conv(conv, beta):\n",
    "    \"\"\"LogSumExp of autoconvolution values.\"\"\"\n",
    "    c_max = np.max(conv)\n",
    "    return c_max + np.log(np.sum(np.exp(np.clip(beta * (conv - c_max), -500.0, 0.0)))) / beta\n",
    "\n",
    "\n",
    "# === Validation: compare FFT vs exact for uniform f ===\n",
    "_e_test = np.linspace(-0.25, 0.25, 11)\n",
    "_h_test = np.ones(10) * 2.0\n",
    "_conv = fft_autoconv(_e_test, _h_test, N_fft=8192)\n",
    "_peak_fft = np.max(_conv)\n",
    "_peak_ex = peak_exact(_e_test, _h_test)\n",
    "print(f'Uniform f=2, P=10:')\n",
    "print(f'  FFT peak:   {_peak_fft:.6f}')\n",
    "print(f'  Exact peak: {_peak_ex:.6f}')\n",
    "print(f'  Error:      {abs(_peak_fft - _peak_ex):.2e}')\n",
    "\n",
    "# Test with a non-uniform grid\n",
    "_e2 = np.array([-0.25, -0.15, -0.05, 0.0, 0.1, 0.25])\n",
    "_h2 = np.array([1.5, 2.5, 3.0, 2.0, 1.0])\n",
    "_h2 = _h2 / np.sum(_h2 * np.diff(_e2))  # normalize\n",
    "_conv2 = fft_autoconv(_e2, _h2, N_fft=16384)\n",
    "_peak_fft2 = np.max(_conv2)\n",
    "_peak_ex2 = peak_exact(_e2, _h2)\n",
    "print(f'Non-uniform P=5:')\n",
    "print(f'  FFT peak:   {_peak_fft2:.6f}')\n",
    "print(f'  Exact peak: {_peak_ex2:.6f}')\n",
    "print(f'  Error:      {abs(_peak_fft2 - _peak_ex2):.2e}')\n",
    "\n",
    "# JIT warmup for backward kernels\n",
    "_n_freq = 8192 // 2 + 1\n",
    "_f_re, _f_im = fourier_coeffs(_e2, _h2, _n_freq)\n",
    "_d_e, _d_h = fourier_backward(_e2, _h2, _f_re, _f_im, _n_freq)\n",
    "_d_pw = np.zeros(5)\n",
    "_dg, _de = reparam_backward(np.zeros(5), np.zeros(5), np.diff(_e2),\n",
    "                             _h2, np.zeros(6), np.zeros(5), _d_pw)\n",
    "print('Backward kernels compiled.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Optimization Wrapper\n",
    "\n",
    "FFT-based objective for L-BFGS-B, multi-start driver with joblib parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization wrapper defined.\n"
     ]
    }
   ],
   "source": [
    "def _smooth_width_ratio(widths):\n",
    "    \"\"\"Smooth approximation to max(w)/min(w) using p-norm ratio.\n",
    "    sr = (mean(w^p) * mean(w^{-p}))^{1/p}\n",
    "       = ((sum w^p)^{1/p} / P^{1/p}) * ((sum w^{-p})^{1/p} / P^{1/p})  [sic]\n",
    "    As p -> inf: (sum w^p)^{1/p} -> max(w), (sum w^{-p})^{-1/p} -> min(w),\n",
    "    and soft_max / soft_min = (mean(w^p) * mean(w^{-p}))^{1/p} -> max/min.\n",
    "    \"\"\"\n",
    "    p = 4.0\n",
    "    P = len(widths)\n",
    "    wp = widths ** p\n",
    "    wn = widths ** (-p)\n",
    "    return (wp.sum() * wn.sum()) ** (1.0 / p) / P ** (2.0 / p)\n",
    "\n",
    "\n",
    "def _smooth_width_ratio_grad(widths, sr, lam, R, p):\n",
    "    \"\"\"Gradient of penalty = lam * log(1 + (sr/R)^2) w.r.t. widths.\"\"\"\n",
    "    S_p = (widths ** p).sum()\n",
    "    S_n = (widths ** (-p)).sum()\n",
    "\n",
    "    # d(penalty)/d(sr) via chain rule\n",
    "    sr_over_R = sr / R\n",
    "    d_penalty_sr = lam * 2.0 * sr_over_R / (R * (1.0 + sr_over_R ** 2))\n",
    "\n",
    "    # log(sr) = (1/p)*(log(S_p) + log(S_n) - 2*log(P))\n",
    "    # d(log sr)/dw_j = w_j^{p-1}/S_p - w_j^{-(p+1)}/S_n\n",
    "    d_sr_w = sr * (widths ** (p - 1) / S_p - widths ** (-(p + 1)) / S_n)\n",
    "\n",
    "    return d_penalty_sr * d_sr_w\n",
    "\n",
    "\n",
    "def make_fft_objective_and_grad(P, beta, lam=0.05, N_fft=16384):\n",
    "    \"\"\"Return callable that returns (objective, gradient) for L-BFGS-B with jac=True.\n",
    "    Analytical gradient via backprop: O(NP + N log N) vs O(NP^2) for finite diffs.\n",
    "    \"\"\"\n",
    "    n_freq = N_fft // 2 + 1\n",
    "    R = 50.0\n",
    "    p = 4.0\n",
    "\n",
    "    def obj_and_grad(theta):\n",
    "        gamma = theta[:P]\n",
    "        eta = theta[P:]\n",
    "\n",
    "        # === Forward pass ===\n",
    "        edges, heights, widths = theta_to_edges_heights(gamma, eta)\n",
    "        f_re, f_im = fourier_coeffs(edges, heights, n_freq)\n",
    "        g_re, g_im = square_complex(f_re, f_im)\n",
    "        G = g_re + 1j * g_im\n",
    "        conv_raw = np.fft.irfft(G, n=N_fft) * N_fft\n",
    "        conv = np.fft.fftshift(conv_raw)\n",
    "\n",
    "        # LSE\n",
    "        c_max = np.max(conv)\n",
    "        shifted = beta * (conv - c_max)\n",
    "        exp_shifted = np.exp(np.clip(shifted, -500.0, 0.0))\n",
    "        sum_exp = np.sum(exp_shifted)\n",
    "        lse_val = c_max + np.log(sum_exp) / beta\n",
    "\n",
    "        # Penalty\n",
    "        sr = _smooth_width_ratio(widths)\n",
    "        penalty = lam * np.log1p((sr / R) ** 2)\n",
    "        obj_val = lse_val + penalty\n",
    "\n",
    "        # === Backward pass ===\n",
    "\n",
    "        # d(LSE)/d(conv) = softmax weights\n",
    "        d_conv = exp_shifted / sum_exp\n",
    "\n",
    "        # Through fftshift (adjoint = ifftshift)\n",
    "        d_conv_raw = np.fft.ifftshift(d_conv)\n",
    "\n",
    "        # Through (N_fft * irfft): adjoint uses rfft with Hermitian scaling\n",
    "        rfft_d = np.fft.rfft(d_conv_raw)\n",
    "        d_G_re = np.zeros(n_freq)\n",
    "        d_G_im = np.zeros(n_freq)\n",
    "        d_G_re[0] = rfft_d[0].real\n",
    "        d_G_re[-1] = rfft_d[-1].real\n",
    "        d_G_re[1:-1] = 2.0 * rfft_d[1:-1].real\n",
    "        d_G_im[1:-1] = 2.0 * rfft_d[1:-1].imag\n",
    "\n",
    "        # Through complex squaring: G = F^2\n",
    "        d_F_re = 2.0 * (d_G_re * f_re + d_G_im * f_im)\n",
    "        d_F_im = 2.0 * (-d_G_re * f_im + d_G_im * f_re)\n",
    "\n",
    "        # Through Fourier coefficients -> d_edges, d_heights\n",
    "        d_edges, d_heights = fourier_backward(\n",
    "            edges, heights, d_F_re, d_F_im, n_freq)\n",
    "\n",
    "        # Penalty gradient w.r.t. widths\n",
    "        d_penalty_widths = _smooth_width_ratio_grad(widths, sr, lam, R, p)\n",
    "\n",
    "        # Through reparametrization -> d_gamma, d_eta\n",
    "        d_gamma, d_eta = reparam_backward(\n",
    "            gamma, eta, widths, heights, d_edges, d_heights, d_penalty_widths)\n",
    "\n",
    "        grad = np.concatenate([d_gamma, d_eta])\n",
    "        return obj_val, grad\n",
    "\n",
    "    return obj_and_grad\n",
    "\n",
    "\n",
    "def theta_to_solution(theta, P):\n",
    "    \"\"\"Extract edges, heights, widths, exact peak, width ratio from theta.\"\"\"\n",
    "    gamma = theta[:P]\n",
    "    eta = theta[P:]\n",
    "    edges, heights, widths = theta_to_edges_heights(gamma, eta)\n",
    "    exact = peak_exact(edges, heights)\n",
    "    w_ratio = widths.max() / widths.min()\n",
    "    return edges, heights, widths, float(exact), float(w_ratio)\n",
    "\n",
    "\n",
    "def init_theta_uniform(P, noise_gamma=0.1, rng=None):\n",
    "    \"\"\"Initialize theta near uniform grid.\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    gamma = rng.normal(0, noise_gamma, size=P)\n",
    "    raw = rng.exponential(1.0, size=P)\n",
    "    eta = np.log(np.expm1(np.maximum(raw, 1e-6)))\n",
    "    return np.concatenate([gamma, eta])\n",
    "\n",
    "\n",
    "def init_theta_from_solution(P, edges, heights, noise=0.01, rng=None):\n",
    "    \"\"\"Warm-start theta from a known solution.\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    widths = np.diff(edges)\n",
    "    assert len(widths) == P, f\"edges has {len(widths)} bins but P={P}\"\n",
    "    assert len(heights) == P, f\"heights has {len(heights)} entries but P={P}\"\n",
    "    # Invert softplus: softplus(gamma) proportional to widths\n",
    "    target_sp = widths * 2 * P  # arbitrary scale, normalization handles it\n",
    "    gamma = np.log(np.expm1(np.maximum(target_sp, 1e-6)))\n",
    "    gamma += rng.normal(0, noise, size=P)\n",
    "    # Heights: softplus(eta) proportional to heights\n",
    "    target_h = np.maximum(heights, 1e-6)\n",
    "    eta = np.log(np.expm1(np.maximum(target_h, 1e-6)))\n",
    "    eta += rng.normal(0, noise, size=P)\n",
    "    return np.concatenate([gamma, eta])\n",
    "\n",
    "\n",
    "def run_single_restart(theta0, P, beta_schedule, lam=0.05,\n",
    "                       maxiter_per_beta=500, N_fft=16384, verbose=False):\n",
    "    \"\"\"One full LSE continuation from theta0.\n",
    "    Returns (fft_peak, theta, width_ratio).\n",
    "    \"\"\"\n",
    "    theta = theta0.copy()\n",
    "    for beta in beta_schedule:\n",
    "        obj_grad = make_fft_objective_and_grad(P, beta, lam, N_fft)\n",
    "        res = minimize(obj_grad, theta, method='L-BFGS-B', jac=True,\n",
    "                       options={'maxiter': maxiter_per_beta,\n",
    "                                'ftol': 1e-12, 'gtol': 1e-8})\n",
    "        theta = res.x\n",
    "        if verbose:\n",
    "            gamma, eta = theta[:P], theta[P:]\n",
    "            edges, heights, widths = theta_to_edges_heights(gamma, eta)\n",
    "            fft_pk = float(np.max(fft_autoconv(edges, heights, N_fft)))\n",
    "            w_ratio = float(widths.max() / widths.min())\n",
    "            print(f'  beta={beta:>7.1f}  fft_peak={fft_pk:.6f}  '\n",
    "                  f'w_ratio={w_ratio:.1f}  nit={res.nit}')\n",
    "\n",
    "    # Final evaluation\n",
    "    gamma, eta = theta[:P], theta[P:]\n",
    "    edges, heights, widths = theta_to_edges_heights(gamma, eta)\n",
    "    fft_peak = float(np.max(fft_autoconv(edges, heights, N_fft)))\n",
    "    w_ratio = float(widths.max() / widths.min())\n",
    "    return fft_peak, theta, w_ratio\n",
    "\n",
    "\n",
    "def run_optimization(P, n_restarts=30, n_jobs=-1, warm_edges=None,\n",
    "                     warm_heights=None, beta_schedule=None, lam=0.05,\n",
    "                     maxiter_per_beta=500, N_fft=16384, verbose=True):\n",
    "    \"\"\"Full multi-start joint optimization.\"\"\"\n",
    "    if beta_schedule is None:\n",
    "        beta_schedule = [1, 2, 4, 8, 15, 30, 60, 100, 150, 250,\n",
    "                         400, 600, 1000, 1500, 2000]\n",
    "\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    # Build initializations\n",
    "    inits = []\n",
    "    n_warm = 0\n",
    "    if warm_edges is not None and warm_heights is not None:\n",
    "        n_warm = max(1, n_restarts // 3)\n",
    "        for i in range(n_warm):\n",
    "            noise = 0.005 * (i + 1) / n_warm\n",
    "            inits.append(init_theta_from_solution(\n",
    "                P, warm_edges, warm_heights, noise=noise, rng=rng))\n",
    "    for _ in range(n_restarts - n_warm):\n",
    "        inits.append(init_theta_uniform(P, noise_gamma=0.1, rng=rng))\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Running {n_restarts} restarts (P={P}, {n_warm} warm, '\n",
    "              f'N_fft={N_fft}, {len(beta_schedule)} beta stages)...')\n",
    "        t0 = time.time()\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=0)(\n",
    "        delayed(run_single_restart)(\n",
    "            inits[i], P, beta_schedule, lam, maxiter_per_beta, N_fft)\n",
    "        for i in range(n_restarts)\n",
    "    )\n",
    "\n",
    "    # Find best by FFT peak\n",
    "    best_fft = np.inf\n",
    "    best_theta = None\n",
    "    all_peaks = []\n",
    "    for i, (fft_pk, theta, w_ratio) in enumerate(results):\n",
    "        all_peaks.append(fft_pk)\n",
    "        if fft_pk < best_fft:\n",
    "            best_fft = fft_pk\n",
    "            best_theta = theta.copy()\n",
    "            if verbose:\n",
    "                print(f'  Restart {i:>3}: fft_peak={fft_pk:.6f}, '\n",
    "                      f'w_ratio={w_ratio:.1f}  <-- best')\n",
    "        elif verbose and i % 10 == 0:\n",
    "            print(f'  Restart {i:>3}: fft_peak={fft_pk:.6f}, '\n",
    "                  f'w_ratio={w_ratio:.1f}')\n",
    "\n",
    "    # Exact verification of best\n",
    "    edges, heights, widths, exact_pk, w_ratio = theta_to_solution(best_theta, P)\n",
    "\n",
    "    if verbose:\n",
    "        elapsed = time.time() - t0\n",
    "        arr = np.array(all_peaks)\n",
    "        print(f'\\nDone in {elapsed:.1f}s.')\n",
    "        print(f'  FFT best:   {best_fft:.6f}')\n",
    "        print(f'  Exact best: {exact_pk:.6f}')\n",
    "        print(f'  Width ratio: {w_ratio:.2f}')\n",
    "        print(f'  Median:     {np.median(arr):.6f}, '\n",
    "              f'Std: {np.std(arr):.6f}')\n",
    "\n",
    "    return exact_pk, best_theta, all_peaks\n",
    "\n",
    "\n",
    "print('Optimization wrapper defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Validation at P=50\n",
    "\n",
    "Single verbose restart to show beta-schedule progression, then full multi-start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single restart demo (P=50), per-beta-stage:\n",
      "  beta=    1.0  fft_peak=2.752771  w_ratio=14.5  nit=300\n",
      "  beta=    2.0  fft_peak=2.287141  w_ratio=16.2  nit=300\n",
      "  beta=    4.0  fft_peak=1.944456  w_ratio=21.9  nit=300\n",
      "  beta=    8.0  fft_peak=1.762682  w_ratio=22.6  nit=300\n",
      "  beta=   15.0  fft_peak=1.662063  w_ratio=23.0  nit=300\n",
      "  beta=   30.0  fft_peak=1.601604  w_ratio=24.8  nit=300\n",
      "  beta=   60.0  fft_peak=1.566377  w_ratio=23.6  nit=300\n",
      "  beta=  100.0  fft_peak=1.552798  w_ratio=24.1  nit=300\n",
      "  beta=  150.0  fft_peak=1.546866  w_ratio=26.3  nit=300\n",
      "  beta=  250.0  fft_peak=1.542602  w_ratio=28.3  nit=300\n",
      "  beta=  400.0  fft_peak=1.537633  w_ratio=26.7  nit=300\n",
      "  beta=  600.0  fft_peak=1.536022  w_ratio=27.1  nit=300\n",
      "  beta= 1000.0  fft_peak=1.534350  w_ratio=26.8  nit=300\n",
      "  beta= 1500.0  fft_peak=1.533935  w_ratio=26.2  nit=300\n",
      "  beta= 2000.0  fft_peak=1.533450  w_ratio=25.9  nit=300\n",
      "  => exact peak = 1.534687, w_ratio = 25.85\n",
      "\n",
      "Full multi-start optimization:\n",
      "Running 30 restarts (P=50, 0 warm, N_fft=8192, 15 beta stages)...\n",
      "  Restart   0: fft_peak=1.532126, w_ratio=24.0  <-- best\n",
      "  Restart   1: fft_peak=1.531877, w_ratio=12.7  <-- best\n",
      "  Restart   2: fft_peak=1.530938, w_ratio=14.4  <-- best\n",
      "  Restart   3: fft_peak=1.529690, w_ratio=28.0  <-- best\n",
      "  Restart   4: fft_peak=1.527052, w_ratio=22.2  <-- best\n",
      "  Restart  10: fft_peak=1.528738, w_ratio=19.5\n",
      "  Restart  20: fft_peak=1.530145, w_ratio=22.3\n",
      "\n",
      "Done in 327.0s.\n",
      "  FFT best:   1.527052\n",
      "  Exact best: 1.528148\n",
      "  Width ratio: 22.19\n",
      "  Median:     1.531577, Std: 0.002771\n",
      "\n",
      "P=50 result:\n",
      "  Exact peak:  1.528148\n",
      "  Width ratio: 22.19\n",
      "  (Uniform grid baseline ~1.522)\n",
      "  WARNING: worse than uniform baseline, check for bugs\n"
     ]
    }
   ],
   "source": [
    "P = 50\n",
    "beta_sched = [1, 2, 4, 8, 15, 30, 60, 100, 150, 250, 400, 600, 1000, 1500, 2000]\n",
    "\n",
    "# --- Demo: single restart with per-stage verbose output ---\n",
    "print('Single restart demo (P=50), per-beta-stage:')\n",
    "theta0 = init_theta_uniform(P, rng=np.random.default_rng(0))\n",
    "_, theta_demo, _ = run_single_restart(\n",
    "    theta0, P, beta_sched, maxiter_per_beta=300, N_fft=8192, verbose=True)\n",
    "edges_demo, heights_demo, widths_demo, exact_demo, wr_demo = theta_to_solution(theta_demo, P)\n",
    "print(f'  => exact peak = {exact_demo:.6f}, w_ratio = {wr_demo:.2f}')\n",
    "\n",
    "# --- Full multi-start ---\n",
    "print(f'\\nFull multi-start optimization:')\n",
    "best_val_50, best_theta_50, all_vals_50 = run_optimization(\n",
    "    P, n_restarts=30, n_jobs=-1,\n",
    "    beta_schedule=beta_sched,\n",
    "    maxiter_per_beta=300, N_fft=8192\n",
    ")\n",
    "\n",
    "edges_50, heights_50, widths_50, exact_50, w_ratio_50 = theta_to_solution(best_theta_50, P)\n",
    "print(f'\\nP={P} result:')\n",
    "print(f'  Exact peak:  {exact_50:.6f}')\n",
    "print(f'  Width ratio: {w_ratio_50:.2f}')\n",
    "print(f'  (Uniform grid baseline ~1.522)')\n",
    "if exact_50 > 1.52:\n",
    "    print('  WARNING: worse than uniform baseline, check for bugs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Scale to P=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 40 restarts (P=100, 0 warm, N_fft=16384, 15 beta stages)...\n",
      "  Restart   0: fft_peak=1.525221, w_ratio=15.6  <-- best\n",
      "  Restart   1: fft_peak=1.521266, w_ratio=15.3  <-- best\n",
      "  Restart   8: fft_peak=1.520583, w_ratio=23.2  <-- best\n",
      "  Restart  10: fft_peak=1.521120, w_ratio=11.6\n",
      "  Restart  20: fft_peak=1.523934, w_ratio=20.8\n",
      "  Restart  30: fft_peak=1.520904, w_ratio=28.0\n",
      "\n",
      "Done in 2228.5s.\n",
      "  FFT best:   1.520583\n",
      "  Exact best: 1.521442\n",
      "  Width ratio: 23.20\n",
      "  Median:     1.525756, Std: 0.002667\n",
      "\n",
      "P=100 result:\n",
      "  Exact peak:  1.521442\n",
      "  Width ratio: 23.20\n"
     ]
    }
   ],
   "source": [
    "P = 100\n",
    "\n",
    "best_val_100, best_theta_100, all_vals_100 = run_optimization(\n",
    "    P, n_restarts=40, n_jobs=-1,\n",
    "    beta_schedule=beta_sched,\n",
    "    maxiter_per_beta=400, N_fft=16384\n",
    ")\n",
    "\n",
    "edges_100, heights_100, widths_100, exact_100, w_ratio_100 = theta_to_solution(best_theta_100, P)\n",
    "print(f'\\nP={P} result:')\n",
    "print(f'  Exact peak:  {exact_100:.6f}')\n",
    "print(f'  Width ratio: {w_ratio_100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Scale to P=200 with warm start\n",
    "\n",
    "Warm-start from the best known P=200 uniform-grid solution in `best_solutions.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm start from P=200 uniform solution: peak=1.509766\n",
      "Running 50 restarts (P=200, 16 warm, N_fft=16384, 21 beta stages)...\n",
      "  Restart   0: fft_peak=1.524770, w_ratio=34.5  <-- best\n",
      "  Restart   1: fft_peak=1.524312, w_ratio=29.1  <-- best\n",
      "  Restart   4: fft_peak=1.523756, w_ratio=28.6  <-- best\n",
      "  Restart  10: fft_peak=1.525200, w_ratio=49.8\n",
      "  Restart  18: fft_peak=1.522363, w_ratio=27.0  <-- best\n",
      "  Restart  19: fft_peak=1.520906, w_ratio=48.2  <-- best\n",
      "  Restart  20: fft_peak=1.520631, w_ratio=25.1  <-- best\n",
      "  Restart  22: fft_peak=1.518482, w_ratio=31.2  <-- best\n",
      "  Restart  27: fft_peak=1.517820, w_ratio=23.1  <-- best\n",
      "  Restart  28: fft_peak=1.517474, w_ratio=19.5  <-- best\n",
      "  Restart  30: fft_peak=1.518305, w_ratio=23.5\n",
      "  Restart  32: fft_peak=1.516921, w_ratio=35.2  <-- best\n",
      "  Restart  40: fft_peak=1.519872, w_ratio=18.2\n",
      "\n",
      "Done in 9898.7s.\n",
      "  FFT best:   1.516921\n",
      "  Exact best: 1.518741\n",
      "  Width ratio: 35.21\n",
      "  Median:     1.522352, Std: 0.002860\n",
      "\n",
      "P=200 result:\n",
      "  Exact peak:   1.518741\n",
      "  Width ratio:  35.21\n",
      "  Baseline:     1.509766\n",
      "  Improvement:  -0.008975\n"
     ]
    }
   ],
   "source": [
    "# Load best known P=200 uniform solution for warm starting\n",
    "with open('best_solutions.json', 'r') as f:\n",
    "    best_solutions = json.load(f)\n",
    "\n",
    "sol200 = best_solutions['heavy_P200']\n",
    "warm_edges_200 = np.array(sol200['edges'])\n",
    "warm_heights_200 = np.array(sol200['heights'])\n",
    "print(f'Warm start from P=200 uniform solution: peak={sol200[\"exact_peak\"]:.6f}')\n",
    "\n",
    "P = 200\n",
    "beta_sched_fine = [1, 1.5, 2, 3, 5, 8, 12, 18, 28, 42, 65, 100, 150, 230,\n",
    "                   350, 500, 750, 1000, 1500, 2000, 3000]\n",
    "\n",
    "best_val_200, best_theta_200, all_vals_200 = run_optimization(\n",
    "    P, n_restarts=50, n_jobs=-1,\n",
    "    warm_edges=warm_edges_200, warm_heights=warm_heights_200,\n",
    "    beta_schedule=beta_sched_fine,\n",
    "    maxiter_per_beta=500, N_fft=16384\n",
    ")\n",
    "\n",
    "edges_200, heights_200, widths_200, exact_200, w_ratio_200 = theta_to_solution(best_theta_200, P)\n",
    "print(f'\\nP={P} result:')\n",
    "print(f'  Exact peak:   {exact_200:.6f}')\n",
    "print(f'  Width ratio:  {w_ratio_200:.2f}')\n",
    "print(f'  Baseline:     {sol200[\"exact_peak\"]:.6f}')\n",
    "print(f'  Improvement:  {sol200[\"exact_peak\"] - exact_200:+.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Summary and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    P |   Exact Peak |  W Ratio | Notes\n",
      "-------------------------------------------------------\n",
      "   50 |     1.528148 |    22.19 | \n",
      "  100 |     1.521442 |    23.20 | \n",
      "  200 |     1.518741 |    35.21 | (uniform baseline: 1.509766)\n",
      "\n",
      "Best overall: P200 -> 1.518741\n",
      "Best known in literature: 1.5029\n",
      "\n",
      "Saved to joint_optimization_results.json\n"
     ]
    }
   ],
   "source": [
    "# Collect results\n",
    "results_all = {}\n",
    "for label, theta, p_val in [\n",
    "    ('P50', best_theta_50, 50),\n",
    "    ('P100', best_theta_100, 100),\n",
    "    ('P200', best_theta_200, 200),\n",
    "]:\n",
    "    e, h, w, ex, wr = theta_to_solution(theta, p_val)\n",
    "    results_all[label] = {\n",
    "        'P': p_val,\n",
    "        'exact_peak': ex,\n",
    "        'width_ratio': wr,\n",
    "        'edges': e.tolist(),\n",
    "        'heights': h.tolist(),\n",
    "    }\n",
    "\n",
    "# Summary table\n",
    "print(f'{\"P\":>5} | {\"Exact Peak\":>12} | {\"W Ratio\":>8} | Notes')\n",
    "print('-' * 55)\n",
    "for label in ['P50', 'P100', 'P200']:\n",
    "    r = results_all[label]\n",
    "    note = ''\n",
    "    if label == 'P200':\n",
    "        note = f'(uniform baseline: {sol200[\"exact_peak\"]:.6f})'\n",
    "    print(f'{r[\"P\"]:>5} | {r[\"exact_peak\"]:>12.6f} | {r[\"width_ratio\"]:>8.2f} | {note}')\n",
    "\n",
    "# Find best\n",
    "best_label = min(results_all, key=lambda k: results_all[k]['exact_peak'])\n",
    "print(f'\\nBest overall: {best_label} -> {results_all[best_label][\"exact_peak\"]:.6f}')\n",
    "print(f'Best known in literature: 1.5029')\n",
    "\n",
    "# Save\n",
    "out_path = 'joint_optimization_results.json'\n",
    "with open(out_path, 'w') as f:\n",
    "    json.dump(results_all, f, indent=2)\n",
    "print(f'\\nSaved to {out_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
