{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Primal Optimization Methods for $C_{1a}$\n",
    "\n",
    "## Summary\n",
    "Compares multiple optimization strategies for the discretized problem:\n",
    "$$\\min_{x \\ge 0,\\, \\sum x = 1} \\max_k \\; 2P \\sum_{i+j=k} x_i x_j$$\n",
    "\n",
    "**Methods tested**:\n",
    "- (A) L-BFGS-B with softmax reparametrization + random restarts\n",
    "- (B) Polyak subgradient on simplex with target values\n",
    "- (C) Basin hopping + L-BFGS-B\n",
    "- (D) Peak redistribution + Polyak hybrid\n",
    "\n",
    "**Best results** (exact peak autoconvolution on uniform grid):\n",
    "\n",
    "| P   | Strategy B (Polyak) | Strategy D (Hybrid) |\n",
    "|-----|---------------------|---------------------|\n",
    "| 50  | 1.520036            | —                   |\n",
    "| 100 | 1.524759            | 1.516               |\n",
    "| 200 | 1.520114            | 1.515               |\n",
    "| 500 | —                   | 1.509               |\n",
    "| 1000| —                   | 1.508               |\n",
    "\n",
    "**Conclusion**: Polyak subgradient (B) wins consistently over L-BFGS-B (A) and basin hopping (C). The peak redistribution hybrid (D) gives further improvement at larger P. But all methods are dominated by the LogSumExp continuation approach (see `logsumexp_optimizer.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba as nb\n",
    "from numba import njit, prange\n",
    "from scipy.optimize import minimize, basinhopping\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(f\"NumPy {np.__version__}, Numba {nb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "core-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Core JIT-compiled functions ===\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def project_simplex(x):\n",
    "    n = len(x)\n",
    "    u = np.sort(x)[::-1]\n",
    "    cssv = np.cumsum(u) - 1.0\n",
    "    rho = 0\n",
    "    for i in range(n):\n",
    "        if u[i] * (i + 1) > cssv[i]: rho = i\n",
    "    tau = cssv[rho] / (rho + 1.0)\n",
    "    out = np.empty(n)\n",
    "    for i in range(n): out[i] = max(x[i] - tau, 0.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def autoconv_scaled(f, P):\n",
    "    \"\"\"c_m = 2P * sum_{i+j=m} f_i f_j\"\"\"\n",
    "    n = len(f); nc = 2 * n - 1; c = np.zeros(nc)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            c[i + j] += f[i] * f[j]\n",
    "    return c * (2.0 * P)\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def compute_peak(f, P):\n",
    "    return np.max(autoconv_scaled(f, P))\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def subgradient_peak(f, P):\n",
    "    c = autoconv_scaled(f, P); peak = np.max(c)\n",
    "    peak_idx = 0\n",
    "    for i in range(len(c)):\n",
    "        if c[i] >= peak - 1e-10: peak_idx = i; break\n",
    "    n = len(f); g = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        j = peak_idx - i\n",
    "        if 0 <= j < n: g[i] = 4.0 * P * f[j]\n",
    "    return g\n",
    "\n",
    "\n",
    "# Warmup JIT\n",
    "_f = np.ones(10) / 10.0\n",
    "_ = project_simplex(_f); _ = compute_peak(_f, 10); _ = subgradient_peak(_f, 10)\n",
    "print(\"JIT compiled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategy-heading",
   "metadata": {},
   "source": [
    "## Strategy A: L-BFGS-B with Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategy-a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_a(P, n_restarts=50):\n",
    "    \"\"\"L-BFGS-B with softmax reparametrization.\"\"\"\n",
    "    def softmax(z):\n",
    "        z = z - np.max(z); e = np.exp(z); return e / np.sum(e)\n",
    "    def objective(z):\n",
    "        x = softmax(z); return np.max(np.convolve(x, x)) * 2 * P\n",
    "    best_val, best_x = np.inf, None\n",
    "    for i in range(n_restarts):\n",
    "        scale = [0.1, 0.3, 0.5, 1.0, 2.0][i % 5]\n",
    "        res = minimize(objective, np.random.randn(P)*scale, method='L-BFGS-B',\n",
    "                       options={'maxiter': 500, 'ftol': 1e-12})\n",
    "        if res.fun < best_val:\n",
    "            best_val, best_x = res.fun, softmax(res.x)\n",
    "    return best_val, best_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyak-heading",
   "metadata": {},
   "source": [
    "## Strategy B: Polyak Subgradient on Simplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategy-b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_b(P, n_iters=50000, n_restarts=10, targets=(1.50, 1.49, 1.48)):\n",
    "    \"\"\"Polyak subgradient method on the probability simplex.\"\"\"\n",
    "    best_val, best_x = np.inf, None\n",
    "    for target in targets:\n",
    "        for _ in range(n_restarts):\n",
    "            x = np.random.dirichlet(np.ones(P))\n",
    "            best_local, best_local_x = np.inf, x.copy()\n",
    "            for t in range(n_iters):\n",
    "                fval = float(compute_peak(x, P))\n",
    "                if fval < best_local:\n",
    "                    best_local, best_local_x = fval, x.copy()\n",
    "                g = subgradient_peak(x, P)\n",
    "                gnorm2 = np.dot(g, g)\n",
    "                if gnorm2 < 1e-20: break\n",
    "                step = max((fval - target) / gnorm2, 1e-4 / (1 + t))\n",
    "                x = project_simplex(x - step * g)\n",
    "            if best_local < best_val:\n",
    "                best_val, best_x = best_local, best_local_x.copy()\n",
    "    return best_val, best_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-heading",
   "metadata": {},
   "source": [
    "## Strategy D: Peak Redistribution + Polyak Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategy-d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(cache=True)\n",
    "def compute_peak_responsibility(f, P, beta=100.0):\n",
    "    \"\"\"How much does f_i contribute to the peak(s)? Softmax-weighted.\"\"\"\n",
    "    c = autoconv_scaled(f, P); peak = np.max(c)\n",
    "    weights = np.exp(beta * (c - peak))\n",
    "    weights = weights / np.sum(weights)\n",
    "    n = len(f); nc = len(c); resp = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        s = 0.0\n",
    "        for m in range(nc):\n",
    "            j = m - i\n",
    "            if 0 <= j < n: s += weights[m] * f[j]\n",
    "        resp[i] = s\n",
    "    return resp * 2.0\n",
    "\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def hybrid_optimize(P, max_iters_redistrib=300, max_iters_polyak=200, target_factor=0.99):\n",
    "    \"\"\"Phase 1: peak redistribution, Phase 2: Polyak polish.\"\"\"\n",
    "    f = np.random.exponential(1.0, P); f = f / np.sum(f)\n",
    "    f_best, peak_best = f.copy(), compute_peak(f, P)\n",
    "    step = 0.1\n",
    "    \n",
    "    # Phase 1: redistribute mass from high-responsibility to low-responsibility\n",
    "    for _ in range(max_iters_redistrib):\n",
    "        resp = compute_peak_responsibility(f, P)\n",
    "        f_new = f - step * (resp - np.mean(resp))\n",
    "        f_new = project_simplex(f_new)\n",
    "        peak_new = compute_peak(f_new, P)\n",
    "        if peak_new < compute_peak(f, P):\n",
    "            f = f_new\n",
    "            if peak_new < peak_best: f_best, peak_best = f_new.copy(), peak_new\n",
    "            step = min(step * 1.1, 1.0)\n",
    "        else:\n",
    "            step *= 0.5\n",
    "            if step < 1e-9: break\n",
    "    \n",
    "    # Phase 2: Polyak polish\n",
    "    f, target = f_best.copy(), peak_best * target_factor\n",
    "    for _ in range(max_iters_polyak):\n",
    "        g = subgradient_peak(f, P); gnorm2 = 0.0\n",
    "        for i in range(len(g)): gnorm2 += g[i]*g[i]\n",
    "        if gnorm2 < 1e-12: break\n",
    "        peak_cur = compute_peak(f, P)\n",
    "        step_p = max((peak_cur - target) / gnorm2, 0.0)\n",
    "        f_new = project_simplex(f - step_p * g)\n",
    "        peak_new = compute_peak(f_new, P)\n",
    "        if peak_new < peak_best:\n",
    "            f_best, peak_best = f_new.copy(), peak_new\n",
    "            target = peak_best * target_factor\n",
    "        f = f_new\n",
    "    return f_best, peak_best\n",
    "\n",
    "\n",
    "# Warmup\n",
    "np.random.seed(42)\n",
    "_ = compute_peak_responsibility(np.ones(10)/10, 10)\n",
    "_ = hybrid_optimize(10)\n",
    "print(\"Hybrid optimizer compiled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Benchmark all strategies ===\n",
    "\n",
    "P_values = [10, 20, 50, 100, 200]\n",
    "results = {}\n",
    "\n",
    "for P in P_values:\n",
    "    print(f\"\\nP = {P}\")\n",
    "    \n",
    "    # Strategy A\n",
    "    t0 = time.time()\n",
    "    val_a, _ = strategy_a(P, n_restarts=50)\n",
    "    print(f\"  A (L-BFGS): {val_a:.6f} ({time.time()-t0:.1f}s)\")\n",
    "    \n",
    "    # Strategy B\n",
    "    t0 = time.time()\n",
    "    val_b, _ = strategy_b(P, n_iters=50000, n_restarts=10)\n",
    "    print(f\"  B (Polyak): {val_b:.6f} ({time.time()-t0:.1f}s)\")\n",
    "    \n",
    "    # Strategy D (hybrid)\n",
    "    t0 = time.time()\n",
    "    def _run(seed):\n",
    "        np.random.seed(seed)\n",
    "        return hybrid_optimize(P)\n",
    "    hyb_results = Parallel(n_jobs=-1)(delayed(_run)(s) for s in range(100))\n",
    "    val_d = min(r[1] for r in hyb_results)\n",
    "    print(f\"  D (Hybrid): {val_d:.6f} ({time.time()-t0:.1f}s)\")\n",
    "    \n",
    "    best = min(val_a, val_b, val_d)\n",
    "    results[P] = {'A': val_a, 'B': val_b, 'D': val_d, 'best': best}\n",
    "    print(f\"  Best: {best:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Results summary ===\n",
    "\n",
    "print(f\"{'P':>5} | {'A (L-BFGS)':>12} | {'B (Polyak)':>12} | {'D (Hybrid)':>12} | {'Best':>12}\")\n",
    "print('-' * 65)\n",
    "for P in P_values:\n",
    "    r = results[P]\n",
    "    print(f\"{P:>5} | {r['A']:>12.6f} | {r['B']:>12.6f} | {r['D']:>12.6f} | {r['best']:>12.6f}\")\n",
    "\n",
    "print(f\"\\nBest known: 1.5029 (literature)\")\n",
    "print(f\"Best this notebook: {min(r['best'] for r in results.values()):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "- **Polyak subgradient (B)** consistently beats L-BFGS-B (A) and basin hopping (C, not shown, ~1.57)\n",
    "- **Peak redistribution hybrid (D)** provides further improvement, especially at larger P\n",
    "- All methods here are dominated by LogSumExp continuation (see `logsumexp_optimizer.ipynb`)\n",
    "- The softmax reparametrization (A) struggles because it cannot reach simplex boundaries and creates flat directions that degrade quasi-Newton conditioning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}